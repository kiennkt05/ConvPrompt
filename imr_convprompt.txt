Train: Epoch[ 1/10]  [ 0/40]  eta: 0:03:08  Lr: 0.001250  Loss: 3.3328  Acc@1: 0.0000 (0.0000)  Acc@5: 17.1875 (17.1875)  time: 4.7044  data: 0.7249  max mem: 6661
Train: Epoch[ 1/10]  [10/40]  eta: 0:00:58  Lr: 0.001250  Loss: 2.1376  Acc@1: 23.4375 (22.3011)  Acc@5: 53.1250 (51.2784)  time: 1.9414  data: 0.0661  max mem: 6674
Train: Epoch[ 1/10]  [20/40]  eta: 0:00:36  Lr: 0.001250  Loss: 1.5106  Acc@1: 37.5000 (35.9375)  Acc@5: 67.1875 (64.6577)  time: 1.6732  data: 0.0002  max mem: 6674
Train: Epoch[ 1/10]  [30/40]  eta: 0:00:17  Lr: 0.001250  Loss: 1.1478  Acc@1: 62.5000 (45.9677)  Acc@5: 85.9375 (72.9335)  time: 1.6715  data: 0.0002  max mem: 6674
Train: Epoch[ 1/10]  [39/40]  eta: 0:00:01  Lr: 0.001250  Loss: 1.0632  Acc@1: 68.7500 (51.8285)  Acc@5: 92.1875 (77.1136)  time: 1.6784  data: 0.0003  max mem: 6674
Train: Epoch[ 1/10] Total time: 0:01:10 (1.7534 s / it)
Averaged stats: Lr: 0.001250  Loss: 1.0632  Acc@1: 68.7500 (51.8285)  Acc@5: 92.1875 (77.1136)
Train: Epoch[ 2/10]  [ 0/40]  eta: 0:01:54  Lr: 0.001219  Loss: 0.9857  Acc@1: 75.0000 (75.0000)  Acc@5: 92.1875 (92.1875)  time: 2.8562  data: 0.9964  max mem: 6674
Train: Epoch[ 2/10]  [10/40]  eta: 0:00:53  Lr: 0.001219  Loss: 0.7070  Acc@1: 81.2500 (80.2557)  Acc@5: 95.3125 (95.1705)  time: 1.7751  data: 0.0908  max mem: 6674
Train: Epoch[ 2/10]  [20/40]  eta: 0:00:34  Lr: 0.001219  Loss: 0.7473  Acc@1: 79.6875 (78.9435)  Acc@5: 95.3125 (94.3452)  time: 1.6784  data: 0.0002  max mem: 6674
Train: Epoch[ 2/10]  [30/40]  eta: 0:00:17  Lr: 0.001219  Loss: 0.5357  Acc@1: 76.5625 (78.2258)  Acc@5: 93.7500 (94.5565)  time: 1.6800  data: 0.0002  max mem: 6674
Train: Epoch[ 2/10]  [39/40]  eta: 0:00:01  Lr: 0.001219  Loss: 0.5633  Acc@1: 79.6875 (79.1585)  Acc@5: 95.3125 (94.7306)  time: 1.6488  data: 0.0002  max mem: 6674
Train: Epoch[ 2/10] Total time: 0:01:07 (1.6955 s / it)
Averaged stats: Lr: 0.001219  Loss: 0.5633  Acc@1: 79.6875 (79.1585)  Acc@5: 95.3125 (94.7306)
Train: Epoch[ 3/10]  [ 0/40]  eta: 0:01:45  Lr: 0.001131  Loss: 0.6488  Acc@1: 82.8125 (82.8125)  Acc@5: 96.8750 (96.8750)  time: 2.6477  data: 0.6975  max mem: 6674
Train: Epoch[ 3/10]  [10/40]  eta: 0:00:52  Lr: 0.001131  Loss: 0.5756  Acc@1: 81.2500 (81.8182)  Acc@5: 98.4375 (97.0170)  time: 1.7645  data: 0.0636  max mem: 6674
Train: Epoch[ 3/10]  [20/40]  eta: 0:00:34  Lr: 0.001131  Loss: 0.4751  Acc@1: 79.6875 (79.9851)  Acc@5: 95.3125 (95.9077)  time: 1.6841  data: 0.0002  max mem: 6674
Train: Epoch[ 3/10]  [30/40]  eta: 0:00:17  Lr: 0.001131  Loss: 0.6645  Acc@1: 81.2500 (81.4012)  Acc@5: 95.3125 (95.8669)  time: 1.6828  data: 0.0002  max mem: 6674
Train: Epoch[ 3/10]  [39/40]  eta: 0:00:01  Lr: 0.001131  Loss: 0.4711  Acc@1: 84.3750 (81.9505)  Acc@5: 96.8750 (95.9890)  time: 1.6570  data: 0.0002  max mem: 6674
Train: Epoch[ 3/10] Total time: 0:01:07 (1.6966 s / it)
Averaged stats: Lr: 0.001131  Loss: 0.4711  Acc@1: 84.3750 (81.9505)  Acc@5: 96.8750 (95.9890)
Train: Epoch[ 4/10]  [ 0/40]  eta: 0:01:50  Lr: 0.000992  Loss: 0.5228  Acc@1: 82.8125 (82.8125)  Acc@5: 95.3125 (95.3125)  time: 2.7697  data: 0.9277  max mem: 6674
Train: Epoch[ 4/10]  [10/40]  eta: 0:00:53  Lr: 0.000992  Loss: 0.5083  Acc@1: 84.3750 (85.3693)  Acc@5: 96.8750 (96.5909)  time: 1.7978  data: 0.0845  max mem: 6674
Train: Epoch[ 4/10]  [20/40]  eta: 0:00:34  Lr: 0.000992  Loss: 0.7217  Acc@1: 84.3750 (85.2679)  Acc@5: 98.4375 (96.8006)  time: 1.6882  data: 0.0002  max mem: 6674
Train: Epoch[ 4/10]  [30/40]  eta: 0:00:17  Lr: 0.000992  Loss: 0.6089  Acc@1: 82.8125 (85.2319)  Acc@5: 98.4375 (96.9254)  time: 1.6780  data: 0.0002  max mem: 6674
Train: Epoch[ 4/10]  [39/40]  eta: 0:00:01  Lr: 0.000992  Loss: 0.7144  Acc@1: 85.1064 (85.2930)  Acc@5: 98.4375 (96.9721)  time: 1.6572  data: 0.0002  max mem: 6674
Train: Epoch[ 4/10] Total time: 0:01:08 (1.7025 s / it)
Averaged stats: Lr: 0.000992  Loss: 0.7144  Acc@1: 85.1064 (85.2930)  Acc@5: 98.4375 (96.9721)
Train: Epoch[ 5/10]  [ 0/40]  eta: 0:01:48  Lr: 0.000818  Loss: 0.3195  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)  time: 2.7214  data: 0.7510  max mem: 6674
Train: Epoch[ 5/10]  [10/40]  eta: 0:00:53  Lr: 0.000818  Loss: 0.4535  Acc@1: 84.3750 (86.0795)  Acc@5: 98.4375 (98.2955)  time: 1.7829  data: 0.0685  max mem: 6674
Train: Epoch[ 5/10]  [20/40]  eta: 0:00:34  Lr: 0.000818  Loss: 0.4005  Acc@1: 85.9375 (85.8631)  Acc@5: 98.4375 (97.7679)  time: 1.6838  data: 0.0003  max mem: 6674
Train: Epoch[ 5/10]  [30/40]  eta: 0:00:17  Lr: 0.000818  Loss: 0.5382  Acc@1: 85.9375 (86.3407)  Acc@5: 96.8750 (97.3790)  time: 1.6792  data: 0.0002  max mem: 6674
Train: Epoch[ 5/10]  [39/40]  eta: 0:00:01  Lr: 0.000818  Loss: 0.7035  Acc@1: 85.9375 (85.8828)  Acc@5: 95.3125 (97.0114)  time: 1.6636  data: 0.0002  max mem: 6674
Train: Epoch[ 5/10] Total time: 0:01:08 (1.7019 s / it)
Averaged stats: Lr: 0.000818  Loss: 0.7035  Acc@1: 85.9375 (85.8828)  Acc@5: 95.3125 (97.0114)
Train: Epoch[ 6/10]  [ 0/40]  eta: 0:01:53  Lr: 0.000625  Loss: 0.6710  Acc@1: 78.1250 (78.1250)  Acc@5: 96.8750 (96.8750)  time: 2.8277  data: 0.8725  max mem: 6674
Train: Epoch[ 6/10]  [10/40]  eta: 0:00:53  Lr: 0.000625  Loss: 0.5092  Acc@1: 85.9375 (85.2273)  Acc@5: 96.8750 (97.3011)  time: 1.7848  data: 0.0795  max mem: 6674
Train: Epoch[ 6/10]  [20/40]  eta: 0:00:34  Lr: 0.000625  Loss: 0.5431  Acc@1: 87.5000 (85.8631)  Acc@5: 96.8750 (97.3958)  time: 1.6776  data: 0.0002  max mem: 6674
Train: Epoch[ 6/10]  [30/40]  eta: 0:00:17  Lr: 0.000625  Loss: 0.3478  Acc@1: 87.5000 (86.3911)  Acc@5: 96.8750 (97.5302)  time: 1.6730  data: 0.0002  max mem: 6674
Train: Epoch[ 6/10]  [39/40]  eta: 0:00:01  Lr: 0.000625  Loss: 0.3891  Acc@1: 87.5000 (87.0232)  Acc@5: 96.8750 (97.5226)  time: 1.6612  data: 0.0002  max mem: 6674
Train: Epoch[ 6/10] Total time: 0:01:08 (1.7005 s / it)
Averaged stats: Lr: 0.000625  Loss: 0.3891  Acc@1: 87.5000 (87.0232)  Acc@5: 96.8750 (97.5226)
Train: Epoch[ 7/10]  [ 0/40]  eta: 0:01:48  Lr: 0.000432  Loss: 0.4734  Acc@1: 85.9375 (85.9375)  Acc@5: 100.0000 (100.0000)  time: 2.7097  data: 0.7587  max mem: 6674
Train: Epoch[ 7/10]  [10/40]  eta: 0:00:53  Lr: 0.000432  Loss: 0.6428  Acc@1: 87.5000 (87.3580)  Acc@5: 98.4375 (97.5852)  time: 1.7679  data: 0.0692  max mem: 6674
Train: Epoch[ 7/10]  [20/40]  eta: 0:00:34  Lr: 0.000432  Loss: 0.3856  Acc@1: 89.0625 (87.7232)  Acc@5: 98.4375 (97.4702)  time: 1.6740  data: 0.0002  max mem: 6674
Train: Epoch[ 7/10]  [30/40]  eta: 0:00:17  Lr: 0.000432  Loss: 0.5517  Acc@1: 89.0625 (87.9536)  Acc@5: 98.4375 (97.4294)  time: 1.6845  data: 0.0002  max mem: 6674
Train: Epoch[ 7/10]  [39/40]  eta: 0:00:01  Lr: 0.000432  Loss: 0.4709  Acc@1: 85.9375 (87.4951)  Acc@5: 96.8750 (97.4440)  time: 1.6638  data: 0.0002  max mem: 6674
Train: Epoch[ 7/10] Total time: 0:01:07 (1.6972 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.4709  Acc@1: 85.9375 (87.4951)  Acc@5: 96.8750 (97.4440)
Train: Epoch[ 8/10]  [ 0/40]  eta: 0:01:46  Lr: 0.000258  Loss: 0.4768  Acc@1: 85.9375 (85.9375)  Acc@5: 95.3125 (95.3125)  time: 2.6549  data: 0.7478  max mem: 6674
Train: Epoch[ 8/10]  [10/40]  eta: 0:00:53  Lr: 0.000258  Loss: 0.2700  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (97.1591)  time: 1.7714  data: 0.0682  max mem: 6674
Train: Epoch[ 8/10]  [20/40]  eta: 0:00:34  Lr: 0.000258  Loss: 0.5737  Acc@1: 89.0625 (87.6488)  Acc@5: 96.8750 (97.3214)  time: 1.6804  data: 0.0002  max mem: 6674
Train: Epoch[ 8/10]  [30/40]  eta: 0:00:17  Lr: 0.000258  Loss: 0.5454  Acc@1: 89.0625 (87.8528)  Acc@5: 96.8750 (97.3286)  time: 1.6838  data: 0.0002  max mem: 6674
Train: Epoch[ 8/10]  [39/40]  eta: 0:00:01  Lr: 0.000258  Loss: 0.6956  Acc@1: 89.0625 (87.9670)  Acc@5: 98.4375 (97.5619)  time: 1.6614  data: 0.0002  max mem: 6674
Train: Epoch[ 8/10] Total time: 0:01:07 (1.6973 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.6956  Acc@1: 89.0625 (87.9670)  Acc@5: 98.4375 (97.5619)
Train: Epoch[ 9/10]  [ 0/40]  eta: 0:01:51  Lr: 0.000119  Loss: 0.2674  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 2.7919  data: 0.8843  max mem: 6674
Train: Epoch[ 9/10]  [10/40]  eta: 0:00:53  Lr: 0.000119  Loss: 0.3984  Acc@1: 89.0625 (88.7784)  Acc@5: 98.4375 (97.5852)  time: 1.7801  data: 0.0806  max mem: 6674
Train: Epoch[ 9/10]  [20/40]  eta: 0:00:34  Lr: 0.000119  Loss: 0.1585  Acc@1: 89.0625 (88.9881)  Acc@5: 98.4375 (97.7679)  time: 1.6865  data: 0.0002  max mem: 6674
Train: Epoch[ 9/10]  [30/40]  eta: 0:00:17  Lr: 0.000119  Loss: 0.4865  Acc@1: 89.0625 (88.9113)  Acc@5: 96.8750 (97.7319)  time: 1.6847  data: 0.0002  max mem: 6674
Train: Epoch[ 9/10]  [39/40]  eta: 0:00:01  Lr: 0.000119  Loss: 0.3810  Acc@1: 89.3617 (89.0287)  Acc@5: 98.4375 (97.7192)  time: 1.6542  data: 0.0002  max mem: 6674
Train: Epoch[ 9/10] Total time: 0:01:08 (1.7000 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.3810  Acc@1: 89.3617 (89.0287)  Acc@5: 98.4375 (97.7192)
Train: Epoch[10/10]  [ 0/40]  eta: 0:01:51  Lr: 0.000031  Loss: 0.5658  Acc@1: 84.3750 (84.3750)  Acc@5: 96.8750 (96.8750)  time: 2.7838  data: 0.7855  max mem: 6674
Train: Epoch[10/10]  [10/40]  eta: 0:00:53  Lr: 0.000031  Loss: 0.3461  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 1.7746  data: 0.0716  max mem: 6674
Train: Epoch[10/10]  [20/40]  eta: 0:00:34  Lr: 0.000031  Loss: 0.3837  Acc@1: 90.6250 (89.7321)  Acc@5: 98.4375 (98.2887)  time: 1.6858  data: 0.0002  max mem: 6674
Train: Epoch[10/10]  [30/40]  eta: 0:00:17  Lr: 0.000031  Loss: 0.3508  Acc@1: 89.0625 (89.4657)  Acc@5: 98.4375 (98.1351)  time: 1.6908  data: 0.0002  max mem: 6674
Train: Epoch[10/10]  [39/40]  eta: 0:00:01  Lr: 0.000031  Loss: 0.4827  Acc@1: 89.0625 (89.5399)  Acc@5: 98.4375 (97.9552)  time: 1.6597  data: 0.0002  max mem: 6674
Train: Epoch[10/10] Total time: 0:01:08 (1.7025 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.4827  Acc@1: 89.0625 (89.5399)  Acc@5: 98.4375 (97.9552)
Test: [Task 1]  [ 0/11]  eta: 0:00:17  Loss: 0.2477 (0.2477)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 1.5679  data: 0.8811  max mem: 6674
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.3494 (0.3406)  Acc@1: 89.0625 (88.9535)  Acc@5: 98.4375 (98.5465)  time: 0.6513  data: 0.0803  max mem: 6674
Test: [Task 1] Total time: 0:00:07 (0.6569 s / it)
* Acc@1 88.953 Acc@5 98.547 loss 0.341
Batchwise eval time for task 1 = 0.6569028334184126
[Average accuracy till task1]	Acc@1: 88.9535	Acc@5: 98.5465	Loss: 0.3406
Eval time for task 1 = 7.259625196456909
Using adam optimizer
Reinitialising optimizer
modules.json: 100%|█████████████████████████████| 229/229 [00:00<00:00, 119kB/s]
config_sentence_transformers.json: 100%|███████| 116/116 [00:00<00:00, 65.9kB/s]
README.md: 5.41kB [00:00, 12.7MB/s]
sentence_bert_config.json: 100%|█████████████| 52.0/52.0 [00:00<00:00, 30.1kB/s]
/kaggle/working/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|██████████████████████████████| 670/670 [00:00<00:00, 373kB/s]
pytorch_model.bin: 100%|█████████████████████| 438M/438M [00:34<00:00, 12.8MB/s]
/kaggle/working/venv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
tokenizer_config.json: 100%|████████████████████| 632/632 [00:00<00:00, 394kB/s]
vocab.txt: 232kB [00:00, 58.8MB/s]
tokenizer.json: 466kB [00:00, 105MB/s]
special_tokens_map.json: 100%|█████████████████| 112/112 [00:00<00:00, 65.2kB/s]
config.json: 100%|█████████████████████████████| 190/190 [00:00<00:00, 23.6kB/s]
Similarity:  tensor(0.8795)  Task:  1
Old Num K:  5 New Num K:  6
Task number:  1
Train: Epoch[ 1/10]  [ 0/46]  eta: 0:02:05  Lr: 0.001250  Loss: 3.2523  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 2.7253  data: 0.6805  max mem: 6730
Train: Epoch[ 1/10]  [10/46]  eta: 0:01:00  Lr: 0.001250  Loss: 2.3849  Acc@1: 3.1250 (5.5398)  Acc@5: 40.6250 (36.7898)  time: 1.6905  data: 0.0621  max mem: 6740
Train: Epoch[ 1/10]  [20/46]  eta: 0:00:42  Lr: 0.001250  Loss: 3.0085  Acc@1: 18.7500 (18.4524)  Acc@5: 62.5000 (53.6458)  time: 1.5894  data: 0.0003  max mem: 6740
Train: Epoch[ 1/10]  [30/46]  eta: 0:00:26  Lr: 0.001250  Loss: 1.6446  Acc@1: 42.1875 (27.7722)  Acc@5: 76.5625 (62.3992)  time: 1.5902  data: 0.0003  max mem: 6740
Train: Epoch[ 1/10]  [40/46]  eta: 0:00:09  Lr: 0.001250  Loss: 2.0186  Acc@1: 50.0000 (34.4512)  Acc@5: 81.2500 (67.2256)  time: 1.5907  data: 0.0002  max mem: 6740
Train: Epoch[ 1/10]  [45/46]  eta: 0:00:01  Lr: 0.001250  Loss: 2.0272  Acc@1: 54.6875 (36.9528)  Acc@5: 82.8125 (68.7349)  time: 1.5756  data: 0.0002  max mem: 6740
Train: Epoch[ 1/10] Total time: 0:01:14 (1.6102 s / it)
Averaged stats: Lr: 0.001250  Loss: 2.0272  Acc@1: 54.6875 (36.9528)  Acc@5: 82.8125 (68.7349)
Train: Epoch[ 2/10]  [ 0/46]  eta: 0:02:01  Lr: 0.001219  Loss: 1.4050  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 2.6363  data: 0.7349  max mem: 6740
Train: Epoch[ 2/10]  [10/46]  eta: 0:01:00  Lr: 0.001219  Loss: 1.8739  Acc@1: 67.1875 (66.7614)  Acc@5: 87.5000 (87.7841)  time: 1.6865  data: 0.0670  max mem: 6741
Train: Epoch[ 2/10]  [20/46]  eta: 0:00:42  Lr: 0.001219  Loss: 1.2246  Acc@1: 65.6250 (66.5179)  Acc@5: 87.5000 (88.3185)  time: 1.5888  data: 0.0002  max mem: 6741
Train: Epoch[ 2/10]  [30/46]  eta: 0:00:25  Lr: 0.001219  Loss: 2.2656  Acc@1: 65.6250 (66.0282)  Acc@5: 87.5000 (88.3065)  time: 1.5881  data: 0.0002  max mem: 6741
Train: Epoch[ 2/10]  [40/46]  eta: 0:00:09  Lr: 0.001219  Loss: 1.3570  Acc@1: 67.1875 (66.1204)  Acc@5: 87.5000 (88.1479)  time: 1.5919  data: 0.0002  max mem: 6741
Train: Epoch[ 2/10]  [45/46]  eta: 0:00:01  Lr: 0.001219  Loss: 1.4533  Acc@1: 67.1875 (66.4254)  Acc@5: 87.5000 (88.2110)  time: 1.5516  data: 0.0002  max mem: 6741
Train: Epoch[ 2/10] Total time: 0:01:13 (1.5975 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.4533  Acc@1: 67.1875 (66.4254)  Acc@5: 87.5000 (88.2110)
Train: Epoch[ 3/10]  [ 0/46]  eta: 0:02:01  Lr: 0.001131  Loss: 1.6634  Acc@1: 81.2500 (81.2500)  Acc@5: 96.8750 (96.8750)  time: 2.6426  data: 0.7038  max mem: 6741
Train: Epoch[ 3/10]  [10/46]  eta: 0:01:00  Lr: 0.001131  Loss: 1.2618  Acc@1: 71.8750 (73.4375)  Acc@5: 93.7500 (92.3295)  time: 1.6894  data: 0.0642  max mem: 6741
Train: Epoch[ 3/10]  [20/46]  eta: 0:00:42  Lr: 0.001131  Loss: 1.5114  Acc@1: 71.8750 (73.4375)  Acc@5: 90.6250 (91.2202)  time: 1.5915  data: 0.0002  max mem: 6741
Train: Epoch[ 3/10]  [30/46]  eta: 0:00:26  Lr: 0.001131  Loss: 1.2403  Acc@1: 75.0000 (74.0927)  Acc@5: 90.6250 (91.0786)  time: 1.5996  data: 0.0002  max mem: 6741
Train: Epoch[ 3/10]  [40/46]  eta: 0:00:09  Lr: 0.001131  Loss: 1.5924  Acc@1: 75.0000 (73.9329)  Acc@5: 90.6250 (91.2729)  time: 1.5961  data: 0.0002  max mem: 6741
Train: Epoch[ 3/10]  [45/46]  eta: 0:00:01  Lr: 0.001131  Loss: 1.2832  Acc@1: 73.4375 (73.6298)  Acc@5: 92.1875 (91.4512)  time: 1.5542  data: 0.0002  max mem: 6741
Train: Epoch[ 3/10] Total time: 0:01:13 (1.6000 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.2832  Acc@1: 73.4375 (73.6298)  Acc@5: 92.1875 (91.4512)
Train: Epoch[ 4/10]  [ 0/46]  eta: 0:02:04  Lr: 0.000992  Loss: 1.0196  Acc@1: 79.6875 (79.6875)  Acc@5: 90.6250 (90.6250)  time: 2.6962  data: 0.7995  max mem: 6741
Train: Epoch[ 4/10]  [10/46]  eta: 0:01:00  Lr: 0.000992  Loss: 1.1885  Acc@1: 78.1250 (78.5511)  Acc@5: 93.7500 (94.1761)  time: 1.6900  data: 0.0729  max mem: 6741
Train: Epoch[ 4/10]  [20/46]  eta: 0:00:42  Lr: 0.000992  Loss: 1.0318  Acc@1: 78.1250 (78.1994)  Acc@5: 93.7500 (93.4524)  time: 1.5928  data: 0.0002  max mem: 6741
Train: Epoch[ 4/10]  [30/46]  eta: 0:00:26  Lr: 0.000992  Loss: 1.3273  Acc@1: 75.0000 (76.8649)  Acc@5: 92.1875 (92.7923)  time: 1.5925  data: 0.0002  max mem: 6741
Train: Epoch[ 4/10]  [40/46]  eta: 0:00:09  Lr: 0.000992  Loss: 1.2238  Acc@1: 71.8750 (75.6479)  Acc@5: 90.6250 (92.5305)  time: 1.5882  data: 0.0003  max mem: 6741
Train: Epoch[ 4/10]  [45/46]  eta: 0:00:01  Lr: 0.000992  Loss: 1.0010  Acc@1: 73.4375 (75.6291)  Acc@5: 90.6250 (92.4509)  time: 1.5472  data: 0.0002  max mem: 6741
Train: Epoch[ 4/10] Total time: 0:01:13 (1.5985 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.0010  Acc@1: 73.4375 (75.6291)  Acc@5: 90.6250 (92.4509)
Train: Epoch[ 5/10]  [ 0/46]  eta: 0:02:03  Lr: 0.000818  Loss: 1.1400  Acc@1: 81.2500 (81.2500)  Acc@5: 95.3125 (95.3125)  time: 2.6878  data: 0.7053  max mem: 6741
Train: Epoch[ 5/10]  [10/46]  eta: 0:01:00  Lr: 0.000818  Loss: 0.9238  Acc@1: 81.2500 (79.4034)  Acc@5: 95.3125 (93.8920)  time: 1.6918  data: 0.0643  max mem: 6741
Train: Epoch[ 5/10]  [20/46]  eta: 0:00:42  Lr: 0.000818  Loss: 1.2274  Acc@1: 79.6875 (79.8363)  Acc@5: 93.7500 (94.1220)  time: 1.5936  data: 0.0002  max mem: 6741
Train: Epoch[ 5/10]  [30/46]  eta: 0:00:26  Lr: 0.000818  Loss: 0.8742  Acc@1: 81.2500 (80.0907)  Acc@5: 95.3125 (94.2540)  time: 1.5920  data: 0.0002  max mem: 6741
Train: Epoch[ 5/10]  [40/46]  eta: 0:00:09  Lr: 0.000818  Loss: 1.3113  Acc@1: 79.6875 (79.2302)  Acc@5: 93.7500 (93.8262)  time: 1.5922  data: 0.0002  max mem: 6741
Train: Epoch[ 5/10]  [45/46]  eta: 0:00:01  Lr: 0.000818  Loss: 0.9553  Acc@1: 79.6875 (79.4898)  Acc@5: 93.7500 (93.8642)  time: 1.5521  data: 0.0002  max mem: 6741
Train: Epoch[ 5/10] Total time: 0:01:13 (1.6007 s / it)
Averaged stats: Lr: 0.000818  Loss: 0.9553  Acc@1: 79.6875 (79.4898)  Acc@5: 93.7500 (93.8642)
Train: Epoch[ 6/10]  [ 0/46]  eta: 0:02:02  Lr: 0.000625  Loss: 0.8654  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 2.6691  data: 0.7636  max mem: 6741
Train: Epoch[ 6/10]  [10/46]  eta: 0:01:00  Lr: 0.000625  Loss: 0.9781  Acc@1: 79.6875 (78.4091)  Acc@5: 93.7500 (93.0398)  time: 1.6888  data: 0.0696  max mem: 6741
Train: Epoch[ 6/10]  [20/46]  eta: 0:00:42  Lr: 0.000625  Loss: 0.9440  Acc@1: 76.5625 (78.1250)  Acc@5: 93.7500 (92.9315)  time: 1.5915  data: 0.0002  max mem: 6741
Train: Epoch[ 6/10]  [30/46]  eta: 0:00:26  Lr: 0.000625  Loss: 1.0766  Acc@1: 78.1250 (79.1331)  Acc@5: 93.7500 (93.3468)  time: 1.6017  data: 0.0003  max mem: 6741
Train: Epoch[ 6/10]  [40/46]  eta: 0:00:09  Lr: 0.000625  Loss: 0.8887  Acc@1: 81.2500 (79.2302)  Acc@5: 93.7500 (93.3689)  time: 1.6001  data: 0.0002  max mem: 6741
Train: Epoch[ 6/10]  [45/46]  eta: 0:00:01  Lr: 0.000625  Loss: 0.9241  Acc@1: 79.6875 (79.3175)  Acc@5: 93.7500 (93.3471)  time: 1.5582  data: 0.0002  max mem: 6741
Train: Epoch[ 6/10] Total time: 0:01:13 (1.6028 s / it)
Averaged stats: Lr: 0.000625  Loss: 0.9241  Acc@1: 79.6875 (79.3175)  Acc@5: 93.7500 (93.3471)
Train: Epoch[ 7/10]  [ 0/46]  eta: 0:02:00  Lr: 0.000432  Loss: 0.9896  Acc@1: 75.0000 (75.0000)  Acc@5: 95.3125 (95.3125)  time: 2.6100  data: 0.7466  max mem: 6741
Train: Epoch[ 7/10]  [10/46]  eta: 0:01:00  Lr: 0.000432  Loss: 0.7221  Acc@1: 79.6875 (78.5511)  Acc@5: 93.7500 (94.0341)  time: 1.6843  data: 0.0681  max mem: 6741
Train: Epoch[ 7/10]  [20/46]  eta: 0:00:42  Lr: 0.000432  Loss: 0.9683  Acc@1: 79.6875 (79.1667)  Acc@5: 93.7500 (93.6756)  time: 1.5921  data: 0.0002  max mem: 6741
Train: Epoch[ 7/10]  [30/46]  eta: 0:00:26  Lr: 0.000432  Loss: 0.7704  Acc@1: 78.1250 (78.8306)  Acc@5: 92.1875 (93.0948)  time: 1.5925  data: 0.0003  max mem: 6741
Train: Epoch[ 7/10]  [40/46]  eta: 0:00:09  Lr: 0.000432  Loss: 0.7210  Acc@1: 78.1250 (78.8872)  Acc@5: 92.1875 (92.5305)  time: 1.5909  data: 0.0003  max mem: 6741
Train: Epoch[ 7/10]  [45/46]  eta: 0:00:01  Lr: 0.000432  Loss: 0.8659  Acc@1: 76.5625 (78.8694)  Acc@5: 92.1875 (92.5543)  time: 1.5491  data: 0.0002  max mem: 6741
Train: Epoch[ 7/10] Total time: 0:01:13 (1.5975 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.8659  Acc@1: 76.5625 (78.8694)  Acc@5: 92.1875 (92.5543)
Train: Epoch[ 8/10]  [ 0/46]  eta: 0:02:03  Lr: 0.000258  Loss: 0.6295  Acc@1: 90.6250 (90.6250)  Acc@5: 93.7500 (93.7500)  time: 2.6747  data: 0.8660  max mem: 6741
Train: Epoch[ 8/10]  [10/46]  eta: 0:01:00  Lr: 0.000258  Loss: 0.5669  Acc@1: 84.3750 (83.6648)  Acc@5: 96.8750 (96.1648)  time: 1.6870  data: 0.0789  max mem: 6741
Train: Epoch[ 8/10]  [20/46]  eta: 0:00:42  Lr: 0.000258  Loss: 0.5248  Acc@1: 79.6875 (81.9196)  Acc@5: 95.3125 (95.5357)  time: 1.5898  data: 0.0002  max mem: 6741
Train: Epoch[ 8/10]  [30/46]  eta: 0:00:26  Lr: 0.000258  Loss: 0.8323  Acc@1: 79.6875 (80.9980)  Acc@5: 95.3125 (95.4637)  time: 1.5917  data: 0.0002  max mem: 6741
Train: Epoch[ 8/10]  [40/46]  eta: 0:00:09  Lr: 0.000258  Loss: 0.5097  Acc@1: 81.2500 (81.0213)  Acc@5: 95.3125 (95.2363)  time: 1.5921  data: 0.0003  max mem: 6741
Train: Epoch[ 8/10]  [45/46]  eta: 0:00:01  Lr: 0.000258  Loss: 0.8088  Acc@1: 79.6875 (80.7997)  Acc@5: 95.3125 (95.0707)  time: 1.5509  data: 0.0002  max mem: 6741
Train: Epoch[ 8/10] Total time: 0:01:13 (1.5987 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.8088  Acc@1: 79.6875 (80.7997)  Acc@5: 95.3125 (95.0707)
Train: Epoch[ 9/10]  [ 0/46]  eta: 0:02:02  Lr: 0.000119  Loss: 0.5775  Acc@1: 82.8125 (82.8125)  Acc@5: 90.6250 (90.6250)  time: 2.6584  data: 0.8575  max mem: 6741
Train: Epoch[ 9/10]  [10/46]  eta: 0:01:00  Lr: 0.000119  Loss: 0.2902  Acc@1: 82.8125 (81.8182)  Acc@5: 93.7500 (94.1761)  time: 1.6910  data: 0.0782  max mem: 6741
Train: Epoch[ 9/10]  [20/46]  eta: 0:00:42  Lr: 0.000119  Loss: 0.4645  Acc@1: 79.6875 (80.7292)  Acc@5: 95.3125 (94.6429)  time: 1.5960  data: 0.0002  max mem: 6741
Train: Epoch[ 9/10]  [30/46]  eta: 0:00:26  Lr: 0.000119  Loss: 0.4299  Acc@1: 79.6875 (80.3931)  Acc@5: 95.3125 (94.6069)  time: 1.6067  data: 0.0002  max mem: 6741
Train: Epoch[ 9/10]  [40/46]  eta: 0:00:09  Lr: 0.000119  Loss: 0.4900  Acc@1: 79.6875 (80.4878)  Acc@5: 95.3125 (94.7027)  time: 1.6025  data: 0.0002  max mem: 6741
Train: Epoch[ 9/10]  [45/46]  eta: 0:00:01  Lr: 0.000119  Loss: 0.4464  Acc@1: 81.2500 (80.8687)  Acc@5: 95.3125 (94.6225)  time: 1.5495  data: 0.0002  max mem: 6741
Train: Epoch[ 9/10] Total time: 0:01:13 (1.6061 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.4464  Acc@1: 81.2500 (80.8687)  Acc@5: 95.3125 (94.6225)
Train: Epoch[10/10]  [ 0/46]  eta: 0:02:02  Lr: 0.000031  Loss: 0.6536  Acc@1: 73.4375 (73.4375)  Acc@5: 89.0625 (89.0625)  time: 2.6615  data: 0.8407  max mem: 6741
Train: Epoch[10/10]  [10/46]  eta: 0:01:00  Lr: 0.000031  Loss: 0.3587  Acc@1: 79.6875 (80.6818)  Acc@5: 96.8750 (94.7443)  time: 1.6937  data: 0.0766  max mem: 6741
Train: Epoch[10/10]  [20/46]  eta: 0:00:42  Lr: 0.000031  Loss: 0.2399  Acc@1: 81.2500 (81.2500)  Acc@5: 95.3125 (94.7917)  time: 1.5965  data: 0.0002  max mem: 6741
Train: Epoch[10/10]  [30/46]  eta: 0:00:26  Lr: 0.000031  Loss: 0.4773  Acc@1: 81.2500 (81.6532)  Acc@5: 95.3125 (95.0605)  time: 1.5960  data: 0.0002  max mem: 6741
Train: Epoch[10/10]  [40/46]  eta: 0:00:09  Lr: 0.000031  Loss: 0.4396  Acc@1: 81.2500 (81.2500)  Acc@5: 95.3125 (94.5503)  time: 1.5939  data: 0.0002  max mem: 6741
Train: Epoch[10/10]  [45/46]  eta: 0:00:01  Lr: 0.000031  Loss: 0.3443  Acc@1: 81.2500 (81.1789)  Acc@5: 93.7500 (94.4157)  time: 1.5537  data: 0.0002  max mem: 6741
Train: Epoch[10/10] Total time: 0:01:13 (1.6023 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.3443  Acc@1: 81.2500 (81.1789)  Acc@5: 93.7500 (94.4157)
Test: [Task 1]  [ 0/11]  eta: 0:00:17  Loss: 0.3611 (0.3611)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 1.5651  data: 0.8661  max mem: 6741
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.4803 (0.4682)  Acc@1: 85.9375 (86.7733)  Acc@5: 98.4375 (96.9477)  time: 0.6319  data: 0.0792  max mem: 6741
Test: [Task 1] Total time: 0:00:07 (0.6396 s / it)
* Acc@1 86.773 Acc@5 96.948 loss 0.468
Batchwise eval time for task 1 = 0.6395696726712313
Test: [Task 2]  [ 0/12]  eta: 0:00:20  Loss: 0.1941 (0.1941)  Acc@1: 95.3125 (95.3125)  Acc@5: 98.4375 (98.4375)  time: 1.6959  data: 1.0220  max mem: 6741
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.5221 (0.5389)  Acc@1: 85.9375 (84.3750)  Acc@5: 96.8750 (95.5966)  time: 0.6517  data: 0.0931  max mem: 6741
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.4560 (0.4940)  Acc@1: 85.9375 (84.3972)  Acc@5: 96.8750 (95.6028)  time: 0.6298  data: 0.0854  max mem: 6741
Test: [Task 2] Total time: 0:00:07 (0.6367 s / it)
* Acc@1 84.397 Acc@5 95.603 loss 0.494
Batchwise eval time for task 2 = 0.636775553226471
[Average accuracy till task2]	Acc@1: 85.5707	Acc@5: 96.2753	Loss: 0.4811	Forgetting: 2.1802	Backward: -2.1802
Eval time for task 2 = 14.750092029571533
Using adam optimizer
Reinitialising optimizer
/kaggle/working/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Similarity:  tensor(0.8354)  Task:  2
Old Num K:  6 New Num K:  7
Task number:  2
Train: Epoch[ 1/10]  [ 0/28]  eta: 0:01:19  Lr: 0.001250  Loss: 3.3380  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 2.8468  data: 0.7055  max mem: 6780
Train: Epoch[ 1/10]  [10/28]  eta: 0:00:32  Lr: 0.001250  Loss: 2.4945  Acc@1: 0.0000 (2.1307)  Acc@5: 31.2500 (27.6989)  time: 1.8189  data: 0.0643  max mem: 6790
Train: Epoch[ 1/10]  [20/28]  eta: 0:00:14  Lr: 0.001250  Loss: 2.8273  Acc@1: 6.2500 (7.7381)  Acc@5: 51.5625 (46.4286)  time: 1.7133  data: 0.0002  max mem: 6790
Train: Epoch[ 1/10]  [27/28]  eta: 0:00:01  Lr: 0.001250  Loss: 2.1792  Acc@1: 15.6250 (12.1769)  Acc@5: 68.7500 (53.9345)  time: 1.6935  data: 0.0002  max mem: 6790
Train: Epoch[ 1/10] Total time: 0:00:48 (1.7443 s / it)
Averaged stats: Lr: 0.001250  Loss: 2.1792  Acc@1: 15.6250 (12.1769)  Acc@5: 68.7500 (53.9345)
Train: Epoch[ 2/10]  [ 0/28]  eta: 0:01:19  Lr: 0.001219  Loss: 2.3708  Acc@1: 42.1875 (42.1875)  Acc@5: 84.3750 (84.3750)  time: 2.8557  data: 0.7403  max mem: 6790
Train: Epoch[ 2/10]  [10/28]  eta: 0:00:32  Lr: 0.001219  Loss: 1.6595  Acc@1: 40.6250 (40.9091)  Acc@5: 82.8125 (82.8125)  time: 1.8152  data: 0.0675  max mem: 6790
Train: Epoch[ 2/10]  [20/28]  eta: 0:00:14  Lr: 0.001219  Loss: 1.8252  Acc@1: 40.6250 (42.2619)  Acc@5: 82.8125 (82.9613)  time: 1.7117  data: 0.0003  max mem: 6790
Train: Epoch[ 2/10]  [27/28]  eta: 0:00:01  Lr: 0.001219  Loss: 1.0323  Acc@1: 48.4375 (45.1465)  Acc@5: 82.8125 (83.8024)  time: 1.6603  data: 0.0002  max mem: 6790
Train: Epoch[ 2/10] Total time: 0:00:48 (1.7192 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.0323  Acc@1: 48.4375 (45.1465)  Acc@5: 82.8125 (83.8024)
Train: Epoch[ 3/10]  [ 0/28]  eta: 0:01:20  Lr: 0.001131  Loss: 1.6916  Acc@1: 48.4375 (48.4375)  Acc@5: 85.9375 (85.9375)  time: 2.8849  data: 0.9142  max mem: 6790
Train: Epoch[ 3/10]  [10/28]  eta: 0:00:32  Lr: 0.001131  Loss: 1.1902  Acc@1: 51.5625 (51.5625)  Acc@5: 85.9375 (86.3636)  time: 1.8207  data: 0.0833  max mem: 6790
Train: Epoch[ 3/10]  [20/28]  eta: 0:00:14  Lr: 0.001131  Loss: 1.7737  Acc@1: 51.5625 (52.2321)  Acc@5: 84.3750 (85.0446)  time: 1.7172  data: 0.0003  max mem: 6790
Train: Epoch[ 3/10]  [27/28]  eta: 0:00:01  Lr: 0.001131  Loss: 1.9149  Acc@1: 51.5625 (52.9006)  Acc@5: 85.9375 (85.6979)  time: 1.6649  data: 0.0002  max mem: 6790
Train: Epoch[ 3/10] Total time: 0:00:48 (1.7245 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.9149  Acc@1: 51.5625 (52.9006)  Acc@5: 85.9375 (85.6979)
Train: Epoch[ 4/10]  [ 0/28]  eta: 0:01:18  Lr: 0.000992  Loss: 1.8020  Acc@1: 54.6875 (54.6875)  Acc@5: 87.5000 (87.5000)  time: 2.8135  data: 0.7895  max mem: 6790
Train: Epoch[ 4/10]  [10/28]  eta: 0:00:32  Lr: 0.000992  Loss: 1.0888  Acc@1: 54.6875 (54.9716)  Acc@5: 87.5000 (88.4943)  time: 1.8319  data: 0.0720  max mem: 6790
Train: Epoch[ 4/10]  [20/28]  eta: 0:00:14  Lr: 0.000992  Loss: 1.5523  Acc@1: 57.8125 (56.4732)  Acc@5: 89.0625 (89.4345)  time: 1.7233  data: 0.0003  max mem: 6790
Train: Epoch[ 4/10]  [27/28]  eta: 0:00:01  Lr: 0.000992  Loss: 1.7056  Acc@1: 57.8125 (56.9787)  Acc@5: 89.0625 (89.5462)  time: 1.6613  data: 0.0002  max mem: 6790
Train: Epoch[ 4/10] Total time: 0:00:48 (1.7262 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.7056  Acc@1: 57.8125 (56.9787)  Acc@5: 89.0625 (89.5462)
Train: Epoch[ 5/10]  [ 0/28]  eta: 0:01:19  Lr: 0.000818  Loss: 1.4009  Acc@1: 57.8125 (57.8125)  Acc@5: 90.6250 (90.6250)  time: 2.8426  data: 0.8389  max mem: 6790
Train: Epoch[ 5/10]  [10/28]  eta: 0:00:32  Lr: 0.000818  Loss: 1.1819  Acc@1: 59.3750 (60.7955)  Acc@5: 90.6250 (91.0511)  time: 1.8176  data: 0.0765  max mem: 6790
Train: Epoch[ 5/10]  [20/28]  eta: 0:00:14  Lr: 0.000818  Loss: 1.3747  Acc@1: 60.9375 (61.5327)  Acc@5: 90.6250 (90.1786)  time: 1.7160  data: 0.0003  max mem: 6790
Train: Epoch[ 5/10]  [27/28]  eta: 0:00:01  Lr: 0.000818  Loss: 1.1212  Acc@1: 62.5000 (62.6651)  Acc@5: 90.6250 (89.9483)  time: 1.6637  data: 0.0002  max mem: 6790
Train: Epoch[ 5/10] Total time: 0:00:48 (1.7213 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.1212  Acc@1: 62.5000 (62.6651)  Acc@5: 90.6250 (89.9483)
Train: Epoch[ 6/10]  [ 0/28]  eta: 0:01:20  Lr: 0.000625  Loss: 1.1715  Acc@1: 59.3750 (59.3750)  Acc@5: 95.3125 (95.3125)  time: 2.8760  data: 0.7473  max mem: 6790
Train: Epoch[ 6/10]  [10/28]  eta: 0:00:32  Lr: 0.000625  Loss: 1.0362  Acc@1: 65.6250 (65.4830)  Acc@5: 92.1875 (91.4773)  time: 1.8259  data: 0.0681  max mem: 6790
Train: Epoch[ 6/10]  [20/28]  eta: 0:00:14  Lr: 0.000625  Loss: 1.1625  Acc@1: 64.0625 (64.0625)  Acc@5: 90.6250 (91.5923)  time: 1.7195  data: 0.0002  max mem: 6790
Train: Epoch[ 6/10]  [27/28]  eta: 0:00:01  Lr: 0.000625  Loss: 1.2188  Acc@1: 64.0625 (64.8478)  Acc@5: 90.6250 (91.2694)  time: 1.6664  data: 0.0002  max mem: 6790
Train: Epoch[ 6/10] Total time: 0:00:48 (1.7262 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.2188  Acc@1: 64.0625 (64.8478)  Acc@5: 90.6250 (91.2694)
Train: Epoch[ 7/10]  [ 0/28]  eta: 0:01:19  Lr: 0.000432  Loss: 1.1641  Acc@1: 57.8125 (57.8125)  Acc@5: 92.1875 (92.1875)  time: 2.8221  data: 0.7393  max mem: 6790
Train: Epoch[ 7/10]  [10/28]  eta: 0:00:33  Lr: 0.000432  Loss: 0.8201  Acc@1: 65.6250 (66.4773)  Acc@5: 92.1875 (92.3295)  time: 1.8396  data: 0.0674  max mem: 6790
Train: Epoch[ 7/10]  [20/28]  eta: 0:00:14  Lr: 0.000432  Loss: 0.7345  Acc@1: 65.6250 (66.8899)  Acc@5: 92.1875 (92.2619)  time: 1.7270  data: 0.0002  max mem: 6790
Train: Epoch[ 7/10]  [27/28]  eta: 0:00:01  Lr: 0.000432  Loss: 1.2558  Acc@1: 68.7500 (66.8007)  Acc@5: 92.1875 (92.0161)  time: 1.6747  data: 0.0002  max mem: 6790
Train: Epoch[ 7/10] Total time: 0:00:48 (1.7292 s / it)
Averaged stats: Lr: 0.000432  Loss: 1.2558  Acc@1: 68.7500 (66.8007)  Acc@5: 92.1875 (92.0161)
Train: Epoch[ 8/10]  [ 0/28]  eta: 0:01:17  Lr: 0.000258  Loss: 0.8138  Acc@1: 68.7500 (68.7500)  Acc@5: 92.1875 (92.1875)  time: 2.7852  data: 0.7934  max mem: 6790
Train: Epoch[ 8/10]  [10/28]  eta: 0:00:32  Lr: 0.000258  Loss: 0.9769  Acc@1: 64.0625 (64.9148)  Acc@5: 90.6250 (90.9091)  time: 1.8130  data: 0.0723  max mem: 6790
Train: Epoch[ 8/10]  [20/28]  eta: 0:00:14  Lr: 0.000258  Loss: 0.6616  Acc@1: 64.0625 (66.1458)  Acc@5: 90.6250 (91.2946)  time: 1.7158  data: 0.0002  max mem: 6790
Train: Epoch[ 8/10]  [27/28]  eta: 0:00:01  Lr: 0.000258  Loss: 1.1827  Acc@1: 62.5000 (66.3412)  Acc@5: 90.6250 (90.6950)  time: 1.6646  data: 0.0002  max mem: 6790
Train: Epoch[ 8/10] Total time: 0:00:48 (1.7205 s / it)
Averaged stats: Lr: 0.000258  Loss: 1.1827  Acc@1: 62.5000 (66.3412)  Acc@5: 90.6250 (90.6950)
Train: Epoch[ 9/10]  [ 0/28]  eta: 0:01:19  Lr: 0.000119  Loss: 0.8194  Acc@1: 62.5000 (62.5000)  Acc@5: 89.0625 (89.0625)  time: 2.8490  data: 0.7953  max mem: 6790
Train: Epoch[ 9/10]  [10/28]  eta: 0:00:32  Lr: 0.000119  Loss: 0.7070  Acc@1: 65.6250 (67.3295)  Acc@5: 92.1875 (91.7614)  time: 1.8235  data: 0.0725  max mem: 6790
Train: Epoch[ 9/10]  [20/28]  eta: 0:00:14  Lr: 0.000119  Loss: 0.3850  Acc@1: 67.1875 (69.1220)  Acc@5: 92.1875 (92.4851)  time: 1.7175  data: 0.0002  max mem: 6790
Train: Epoch[ 9/10]  [27/28]  eta: 0:00:01  Lr: 0.000119  Loss: 0.6008  Acc@1: 67.1875 (68.1218)  Acc@5: 93.7500 (92.4182)  time: 1.6601  data: 0.0002  max mem: 6790
Train: Epoch[ 9/10] Total time: 0:00:48 (1.7215 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.6008  Acc@1: 67.1875 (68.1218)  Acc@5: 93.7500 (92.4182)
Train: Epoch[10/10]  [ 0/28]  eta: 0:01:20  Lr: 0.000031  Loss: 0.5476  Acc@1: 76.5625 (76.5625)  Acc@5: 92.1875 (92.1875)  time: 2.8705  data: 0.7435  max mem: 6790
Train: Epoch[10/10]  [10/28]  eta: 0:00:32  Lr: 0.000031  Loss: 0.5956  Acc@1: 70.3125 (71.1648)  Acc@5: 93.7500 (92.4716)  time: 1.8280  data: 0.0678  max mem: 6790
Train: Epoch[10/10]  [20/28]  eta: 0:00:14  Lr: 0.000031  Loss: 0.6810  Acc@1: 67.1875 (67.7083)  Acc@5: 90.6250 (91.2202)  time: 1.7221  data: 0.0002  max mem: 6790
Train: Epoch[10/10]  [27/28]  eta: 0:00:01  Lr: 0.000031  Loss: 0.1189  Acc@1: 64.0625 (67.1453)  Acc@5: 90.6250 (91.7289)  time: 1.6674  data: 0.0002  max mem: 6790
Train: Epoch[10/10] Total time: 0:00:48 (1.7275 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.1189  Acc@1: 64.0625 (67.1453)  Acc@5: 90.6250 (91.7289)
Test: [Task 1]  [ 0/11]  eta: 0:00:17  Loss: 0.3790 (0.3790)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 1.5496  data: 0.8453  max mem: 6790
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.5147 (0.5183)  Acc@1: 85.9375 (86.3372)  Acc@5: 96.8750 (96.3663)  time: 0.6531  data: 0.0771  max mem: 6790
Test: [Task 1] Total time: 0:00:07 (0.6609 s / it)
* Acc@1 86.337 Acc@5 96.366 loss 0.518
Batchwise eval time for task 1 = 0.6608954776417125
Test: [Task 2]  [ 0/12]  eta: 0:00:19  Loss: 0.2414 (0.2414)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.6444  data: 0.9783  max mem: 6790
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.5513 (0.5940)  Acc@1: 85.9375 (84.8011)  Acc@5: 96.8750 (95.4545)  time: 0.6673  data: 0.0892  max mem: 6790
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.4981 (0.5447)  Acc@1: 85.9375 (84.8227)  Acc@5: 96.8750 (95.4610)  time: 0.6233  data: 0.0817  max mem: 6790
Test: [Task 2] Total time: 0:00:07 (0.6309 s / it)
* Acc@1 84.823 Acc@5 95.461 loss 0.545
Batchwise eval time for task 2 = 0.6308798988660177
Test: [Task 3]  [0/7]  eta: 0:00:11  Loss: 0.8752 (0.8752)  Acc@1: 75.0000 (75.0000)  Acc@5: 90.6250 (90.6250)  time: 1.6402  data: 1.0341  max mem: 6790
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 0.8201 (0.8751)  Acc@1: 81.2500 (77.4194)  Acc@5: 95.3125 (94.0092)  time: 0.7783  data: 0.1479  max mem: 6790
Test: [Task 3] Total time: 0:00:05 (0.7909 s / it)
* Acc@1 77.419 Acc@5 94.009 loss 0.875
Batchwise eval time for task 3 = 0.7909442016056606
[Average accuracy till task3]	Acc@1: 83.6344	Acc@5: 95.2788	Loss: 0.6460	Forgetting: 1.3081	Backward: -1.0954
Eval time for task 3 = 20.48281741142273
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.8709)  Task:  3
Old Num K:  7 New Num K:  8
Task number:  3
Train: Epoch[ 1/10]  [ 0/37]  eta: 0:02:08  Lr: 0.001250  Loss: 3.1772  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 3.4705  data: 1.5452  max mem: 6832
Train: Epoch[ 1/10]  [10/37]  eta: 0:00:53  Lr: 0.001250  Loss: 2.3997  Acc@1: 0.0000 (2.9830)  Acc@5: 9.3750 (16.7614)  time: 1.9907  data: 0.1407  max mem: 6843
Train: Epoch[ 1/10]  [20/37]  eta: 0:00:32  Lr: 0.001250  Loss: 2.9232  Acc@1: 10.9375 (13.8393)  Acc@5: 40.6250 (34.7470)  time: 1.8386  data: 0.0002  max mem: 6843
Train: Epoch[ 1/10]  [30/37]  eta: 0:00:13  Lr: 0.001250  Loss: 1.6462  Acc@1: 31.2500 (21.3206)  Acc@5: 60.9375 (46.1694)  time: 1.8445  data: 0.0003  max mem: 6843
Train: Epoch[ 1/10]  [36/37]  eta: 0:00:01  Lr: 0.001250  Loss: 2.1808  Acc@1: 37.5000 (24.8279)  Acc@5: 68.7500 (50.3012)  time: 1.8193  data: 0.0002  max mem: 6843
Train: Epoch[ 1/10] Total time: 0:01:09 (1.8756 s / it)
Averaged stats: Lr: 0.001250  Loss: 2.1808  Acc@1: 37.5000 (24.8279)  Acc@5: 68.7500 (50.3012)
Train: Epoch[ 2/10]  [ 0/37]  eta: 0:01:54  Lr: 0.001219  Loss: 2.0564  Acc@1: 51.5625 (51.5625)  Acc@5: 84.3750 (84.3750)  time: 3.1056  data: 0.7743  max mem: 6843
Train: Epoch[ 2/10]  [10/37]  eta: 0:00:52  Lr: 0.001219  Loss: 1.5034  Acc@1: 53.1250 (53.4091)  Acc@5: 82.8125 (81.8182)  time: 1.9503  data: 0.0706  max mem: 6843
Train: Epoch[ 2/10]  [20/37]  eta: 0:00:32  Lr: 0.001219  Loss: 1.7490  Acc@1: 54.6875 (54.8363)  Acc@5: 81.2500 (81.6964)  time: 1.8374  data: 0.0002  max mem: 6843
Train: Epoch[ 2/10]  [30/37]  eta: 0:00:13  Lr: 0.001219  Loss: 1.5820  Acc@1: 57.8125 (55.0907)  Acc@5: 79.6875 (81.2500)  time: 1.8393  data: 0.0002  max mem: 6843
Train: Epoch[ 2/10]  [36/37]  eta: 0:00:01  Lr: 0.001219  Loss: 1.2980  Acc@1: 59.3750 (55.6368)  Acc@5: 81.2500 (81.3683)  time: 1.7872  data: 0.0002  max mem: 6843
Train: Epoch[ 2/10] Total time: 0:01:08 (1.8472 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.2980  Acc@1: 59.3750 (55.6368)  Acc@5: 81.2500 (81.3683)
Train: Epoch[ 3/10]  [ 0/37]  eta: 0:01:48  Lr: 0.001131  Loss: 1.0641  Acc@1: 62.5000 (62.5000)  Acc@5: 89.0625 (89.0625)  time: 2.9295  data: 0.7122  max mem: 6843
Train: Epoch[ 3/10]  [10/37]  eta: 0:00:52  Lr: 0.001131  Loss: 1.5854  Acc@1: 64.0625 (62.6420)  Acc@5: 87.5000 (87.5000)  time: 1.9382  data: 0.0650  max mem: 6843
Train: Epoch[ 3/10]  [20/37]  eta: 0:00:32  Lr: 0.001131  Loss: 1.2399  Acc@1: 64.0625 (63.3929)  Acc@5: 87.5000 (87.7232)  time: 1.8385  data: 0.0002  max mem: 6843
Train: Epoch[ 3/10]  [30/37]  eta: 0:00:13  Lr: 0.001131  Loss: 1.6685  Acc@1: 64.0625 (63.6089)  Acc@5: 85.9375 (87.1472)  time: 1.8494  data: 0.0003  max mem: 6843
Train: Epoch[ 3/10]  [36/37]  eta: 0:00:01  Lr: 0.001131  Loss: 1.0562  Acc@1: 64.0625 (63.2530)  Acc@5: 85.9375 (86.5318)  time: 1.7971  data: 0.0002  max mem: 6843
Train: Epoch[ 3/10] Total time: 0:01:08 (1.8488 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.0562  Acc@1: 64.0625 (63.2530)  Acc@5: 85.9375 (86.5318)
Train: Epoch[ 4/10]  [ 0/37]  eta: 0:01:52  Lr: 0.000992  Loss: 1.3939  Acc@1: 56.2500 (56.2500)  Acc@5: 82.8125 (82.8125)  time: 3.0502  data: 0.8553  max mem: 6843
Train: Epoch[ 4/10]  [10/37]  eta: 0:00:52  Lr: 0.000992  Loss: 1.2983  Acc@1: 68.7500 (65.9091)  Acc@5: 89.0625 (87.9261)  time: 1.9464  data: 0.0780  max mem: 6843
Train: Epoch[ 4/10]  [20/37]  eta: 0:00:32  Lr: 0.000992  Loss: 1.3126  Acc@1: 68.7500 (67.7083)  Acc@5: 89.0625 (88.1696)  time: 1.8368  data: 0.0002  max mem: 6843
Train: Epoch[ 4/10]  [30/37]  eta: 0:00:13  Lr: 0.000992  Loss: 1.2312  Acc@1: 68.7500 (67.7923)  Acc@5: 89.0625 (88.4073)  time: 1.8352  data: 0.0002  max mem: 6843
Train: Epoch[ 4/10]  [36/37]  eta: 0:00:01  Lr: 0.000992  Loss: 1.4724  Acc@1: 68.7500 (68.1583)  Acc@5: 90.0000 (88.8554)  time: 1.7812  data: 0.0002  max mem: 6843
Train: Epoch[ 4/10] Total time: 0:01:08 (1.8422 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.4724  Acc@1: 68.7500 (68.1583)  Acc@5: 90.0000 (88.8554)
Train: Epoch[ 5/10]  [ 0/37]  eta: 0:01:50  Lr: 0.000818  Loss: 1.3652  Acc@1: 76.5625 (76.5625)  Acc@5: 90.6250 (90.6250)  time: 2.9849  data: 0.8484  max mem: 6843
Train: Epoch[ 5/10]  [10/37]  eta: 0:00:52  Lr: 0.000818  Loss: 1.1453  Acc@1: 70.3125 (71.4489)  Acc@5: 90.6250 (90.6250)  time: 1.9442  data: 0.0773  max mem: 6843
Train: Epoch[ 5/10]  [20/37]  eta: 0:00:32  Lr: 0.000818  Loss: 1.1807  Acc@1: 70.3125 (70.9821)  Acc@5: 89.0625 (89.8065)  time: 1.8362  data: 0.0002  max mem: 6843
Train: Epoch[ 5/10]  [30/37]  eta: 0:00:13  Lr: 0.000818  Loss: 0.9171  Acc@1: 70.3125 (70.7157)  Acc@5: 89.0625 (90.3730)  time: 1.8433  data: 0.0002  max mem: 6843
Train: Epoch[ 5/10]  [36/37]  eta: 0:00:01  Lr: 0.000818  Loss: 1.1052  Acc@1: 68.7500 (70.7831)  Acc@5: 90.0000 (90.3184)  time: 1.7923  data: 0.0002  max mem: 6843
Train: Epoch[ 5/10] Total time: 0:01:08 (1.8472 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.1052  Acc@1: 68.7500 (70.7831)  Acc@5: 90.0000 (90.3184)
Train: Epoch[ 6/10]  [ 0/37]  eta: 0:01:47  Lr: 0.000625  Loss: 1.1024  Acc@1: 67.1875 (67.1875)  Acc@5: 87.5000 (87.5000)  time: 2.9140  data: 0.7046  max mem: 6843
Train: Epoch[ 6/10]  [10/37]  eta: 0:00:52  Lr: 0.000625  Loss: 0.8117  Acc@1: 71.8750 (72.1591)  Acc@5: 89.0625 (89.9148)  time: 1.9342  data: 0.0643  max mem: 6843
Train: Epoch[ 6/10]  [20/37]  eta: 0:00:32  Lr: 0.000625  Loss: 1.0121  Acc@1: 71.8750 (72.9911)  Acc@5: 89.0625 (90.1786)  time: 1.8379  data: 0.0002  max mem: 6843
Train: Epoch[ 6/10]  [30/37]  eta: 0:00:13  Lr: 0.000625  Loss: 0.9576  Acc@1: 75.0000 (72.9839)  Acc@5: 90.6250 (90.8266)  time: 1.8374  data: 0.0002  max mem: 6843
Train: Epoch[ 6/10]  [36/37]  eta: 0:00:01  Lr: 0.000625  Loss: 1.0084  Acc@1: 70.0000 (71.7298)  Acc@5: 90.6250 (90.4905)  time: 1.7849  data: 0.0002  max mem: 6843
Train: Epoch[ 6/10] Total time: 0:01:08 (1.8403 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.0084  Acc@1: 70.0000 (71.7298)  Acc@5: 90.6250 (90.4905)
Train: Epoch[ 7/10]  [ 0/37]  eta: 0:01:50  Lr: 0.000432  Loss: 0.9284  Acc@1: 62.5000 (62.5000)  Acc@5: 92.1875 (92.1875)  time: 2.9956  data: 0.7477  max mem: 6843
Train: Epoch[ 7/10]  [10/37]  eta: 0:00:52  Lr: 0.000432  Loss: 1.0268  Acc@1: 71.8750 (71.3068)  Acc@5: 90.6250 (89.9148)  time: 1.9406  data: 0.0682  max mem: 6843
Train: Epoch[ 7/10]  [20/37]  eta: 0:00:32  Lr: 0.000432  Loss: 0.6025  Acc@1: 73.4375 (72.5446)  Acc@5: 90.6250 (90.4018)  time: 1.8499  data: 0.0002  max mem: 6843
Train: Epoch[ 7/10]  [30/37]  eta: 0:00:13  Lr: 0.000432  Loss: 0.8277  Acc@1: 73.4375 (72.3286)  Acc@5: 90.6250 (90.0202)  time: 1.8541  data: 0.0003  max mem: 6843
Train: Epoch[ 7/10]  [36/37]  eta: 0:00:01  Lr: 0.000432  Loss: 0.5013  Acc@1: 70.3125 (72.0740)  Acc@5: 89.0625 (89.9742)  time: 1.8016  data: 0.0002  max mem: 6843
Train: Epoch[ 7/10] Total time: 0:01:08 (1.8525 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.5013  Acc@1: 70.3125 (72.0740)  Acc@5: 89.0625 (89.9742)
Train: Epoch[ 8/10]  [ 0/37]  eta: 0:01:44  Lr: 0.000258  Loss: 0.7606  Acc@1: 67.1875 (67.1875)  Acc@5: 93.7500 (93.7500)  time: 2.8361  data: 0.6794  max mem: 6843
Train: Epoch[ 8/10]  [10/37]  eta: 0:00:52  Lr: 0.000258  Loss: 0.6504  Acc@1: 75.0000 (72.5852)  Acc@5: 90.6250 (90.4830)  time: 1.9343  data: 0.0620  max mem: 6843
Train: Epoch[ 8/10]  [20/37]  eta: 0:00:32  Lr: 0.000258  Loss: 0.6998  Acc@1: 73.4375 (72.3958)  Acc@5: 90.6250 (90.8482)  time: 1.8424  data: 0.0002  max mem: 6843
Train: Epoch[ 8/10]  [30/37]  eta: 0:00:13  Lr: 0.000258  Loss: 0.6862  Acc@1: 70.3125 (72.1270)  Acc@5: 92.1875 (90.7258)  time: 1.8423  data: 0.0002  max mem: 6843
Train: Epoch[ 8/10]  [36/37]  eta: 0:00:01  Lr: 0.000258  Loss: 0.5651  Acc@1: 71.8750 (72.2892)  Acc@5: 90.6250 (90.8778)  time: 1.7898  data: 0.0002  max mem: 6843
Train: Epoch[ 8/10] Total time: 0:01:08 (1.8439 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.5651  Acc@1: 71.8750 (72.2892)  Acc@5: 90.6250 (90.8778)
Train: Epoch[ 9/10]  [ 0/37]  eta: 0:01:50  Lr: 0.000119  Loss: 0.6590  Acc@1: 75.0000 (75.0000)  Acc@5: 90.6250 (90.6250)  time: 2.9926  data: 0.7906  max mem: 6843
Train: Epoch[ 9/10]  [10/37]  eta: 0:00:52  Lr: 0.000119  Loss: 0.5997  Acc@1: 71.8750 (72.7273)  Acc@5: 90.6250 (91.3352)  time: 1.9462  data: 0.0721  max mem: 6843
Train: Epoch[ 9/10]  [20/37]  eta: 0:00:32  Lr: 0.000119  Loss: 0.5873  Acc@1: 71.8750 (73.4375)  Acc@5: 92.1875 (91.4435)  time: 1.8539  data: 0.0002  max mem: 6843
Train: Epoch[ 9/10]  [30/37]  eta: 0:00:13  Lr: 0.000119  Loss: 0.8443  Acc@1: 73.4375 (73.0343)  Acc@5: 92.1875 (91.3306)  time: 1.8536  data: 0.0002  max mem: 6843
Train: Epoch[ 9/10]  [36/37]  eta: 0:00:01  Lr: 0.000119  Loss: 0.2579  Acc@1: 73.4375 (73.3219)  Acc@5: 90.6250 (90.9208)  time: 1.8022  data: 0.0002  max mem: 6843
Train: Epoch[ 9/10] Total time: 0:01:08 (1.8546 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.2579  Acc@1: 73.4375 (73.3219)  Acc@5: 90.6250 (90.9208)
Train: Epoch[10/10]  [ 0/37]  eta: 0:01:52  Lr: 0.000031  Loss: 0.4522  Acc@1: 75.0000 (75.0000)  Acc@5: 92.1875 (92.1875)  time: 3.0335  data: 0.8075  max mem: 6843
Train: Epoch[10/10]  [10/37]  eta: 0:00:52  Lr: 0.000031  Loss: 0.3899  Acc@1: 78.1250 (76.1364)  Acc@5: 93.7500 (92.6136)  time: 1.9578  data: 0.0736  max mem: 6843
Train: Epoch[10/10]  [20/37]  eta: 0:00:32  Lr: 0.000031  Loss: 0.5091  Acc@1: 73.4375 (75.2976)  Acc@5: 92.1875 (92.4851)  time: 1.8487  data: 0.0002  max mem: 6843
Train: Epoch[10/10]  [30/37]  eta: 0:00:13  Lr: 0.000031  Loss: 0.4238  Acc@1: 71.8750 (74.5464)  Acc@5: 92.1875 (92.0867)  time: 1.8464  data: 0.0002  max mem: 6843
Train: Epoch[10/10]  [36/37]  eta: 0:00:01  Lr: 0.000031  Loss: 0.6028  Acc@1: 70.3125 (74.6127)  Acc@5: 92.1875 (92.0826)  time: 1.7919  data: 0.0002  max mem: 6843
Train: Epoch[10/10] Total time: 0:01:08 (1.8540 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.6028  Acc@1: 70.3125 (74.6127)  Acc@5: 92.1875 (92.0826)
Test: [Task 1]  [ 0/11]  eta: 0:00:18  Loss: 0.4148 (0.4148)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 1.6505  data: 0.8695  max mem: 6843
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.5826 (0.5761)  Acc@1: 82.8125 (85.3198)  Acc@5: 96.8750 (96.0756)  time: 0.6842  data: 0.0795  max mem: 6843
Test: [Task 1] Total time: 0:00:07 (0.6917 s / it)
* Acc@1 85.320 Acc@5 96.076 loss 0.576
Batchwise eval time for task 1 = 0.691764159636064
Test: [Task 2]  [ 0/12]  eta: 0:00:20  Loss: 0.2707 (0.2707)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.6861  data: 0.9613  max mem: 6843
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.6067 (0.6524)  Acc@1: 85.9375 (83.6648)  Acc@5: 95.3125 (94.6023)  time: 0.6958  data: 0.0876  max mem: 6843
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.5424 (0.5983)  Acc@1: 85.9375 (83.6879)  Acc@5: 95.3125 (94.6099)  time: 0.6525  data: 0.0803  max mem: 6843
Test: [Task 2] Total time: 0:00:07 (0.6599 s / it)
* Acc@1 83.688 Acc@5 94.610 loss 0.598
Batchwise eval time for task 2 = 0.6599205732345581
Test: [Task 3]  [0/7]  eta: 0:00:11  Loss: 0.9808 (0.9808)  Acc@1: 73.4375 (73.4375)  Acc@5: 90.6250 (90.6250)  time: 1.6897  data: 1.0416  max mem: 6843
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 0.8781 (1.0010)  Acc@1: 78.1250 (74.6544)  Acc@5: 93.7500 (93.3180)  time: 0.7364  data: 0.1490  max mem: 6843
Test: [Task 3] Total time: 0:00:05 (0.7498 s / it)
* Acc@1 74.654 Acc@5 93.318 loss 1.001
Batchwise eval time for task 3 = 0.7498283726828439
Test: [Task 4]  [ 0/10]  eta: 0:00:17  Loss: 0.8992 (0.8992)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.1875 (92.1875)  time: 1.7291  data: 0.9136  max mem: 6843
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 0.8274 (0.8273)  Acc@1: 76.5625 (78.1609)  Acc@5: 93.7500 (93.9245)  time: 0.7260  data: 0.0916  max mem: 6843
Test: [Task 4] Total time: 0:00:07 (0.7342 s / it)
* Acc@1 78.161 Acc@5 93.924 loss 0.827
Batchwise eval time for task 4 = 0.7341953754425049
[Average accuracy till task4]	Acc@1: 81.1576	Acc@5: 94.4820	Loss: 0.7507	Forgetting: 2.5111	Backward: -2.3693
Eval time for task 4 = 28.262195348739624
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.8781)  Task:  4
Old Num K:  8 New Num K:  9
Task number:  4
Train: Epoch[ 1/10]  [ 0/45]  eta: 0:02:27  Lr: 0.001250  Loss: 3.3100  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 3.2732  data: 1.0168  max mem: 6886
Train: Epoch[ 1/10]  [10/45]  eta: 0:01:13  Lr: 0.001250  Loss: 2.8625  Acc@1: 0.0000 (1.8466)  Acc@5: 14.0625 (12.2159)  time: 2.1056  data: 0.0927  max mem: 6899
Train: Epoch[ 1/10]  [20/45]  eta: 0:00:51  Lr: 0.001250  Loss: 2.9826  Acc@1: 10.9375 (11.8304)  Acc@5: 32.8125 (29.0179)  time: 1.9784  data: 0.0002  max mem: 6899
Train: Epoch[ 1/10]  [30/45]  eta: 0:00:30  Lr: 0.001250  Loss: 1.9513  Acc@1: 29.6875 (20.3125)  Acc@5: 53.1250 (40.6250)  time: 1.9677  data: 0.0002  max mem: 6899
Train: Epoch[ 1/10]  [40/45]  eta: 0:00:10  Lr: 0.001250  Loss: 2.0159  Acc@1: 43.7500 (27.2485)  Acc@5: 67.1875 (47.8659)  time: 1.9658  data: 0.0002  max mem: 6899
Train: Epoch[ 1/10]  [44/45]  eta: 0:00:02  Lr: 0.001250  Loss: 1.9729  Acc@1: 48.4375 (29.4241)  Acc@5: 67.1875 (49.8429)  time: 1.9893  data: 0.0002  max mem: 6899
Train: Epoch[ 1/10] Total time: 0:01:30 (2.0130 s / it)
Averaged stats: Lr: 0.001250  Loss: 1.9729  Acc@1: 48.4375 (29.4241)  Acc@5: 67.1875 (49.8429)
Train: Epoch[ 2/10]  [ 0/45]  eta: 0:02:34  Lr: 0.001219  Loss: 2.1536  Acc@1: 53.1250 (53.1250)  Acc@5: 71.8750 (71.8750)  time: 3.4252  data: 0.7564  max mem: 6899
Train: Epoch[ 2/10]  [10/45]  eta: 0:01:13  Lr: 0.001219  Loss: 1.5964  Acc@1: 56.2500 (56.6761)  Acc@5: 75.0000 (75.9943)  time: 2.0997  data: 0.0690  max mem: 6899
Train: Epoch[ 2/10]  [20/45]  eta: 0:00:50  Lr: 0.001219  Loss: 1.8049  Acc@1: 57.8125 (57.8869)  Acc@5: 78.1250 (76.8601)  time: 1.9692  data: 0.0002  max mem: 6899
Train: Epoch[ 2/10]  [30/45]  eta: 0:00:30  Lr: 0.001219  Loss: 1.5658  Acc@1: 60.9375 (59.0726)  Acc@5: 79.6875 (77.8226)  time: 1.9662  data: 0.0002  max mem: 6899
Train: Epoch[ 2/10]  [40/45]  eta: 0:00:10  Lr: 0.001219  Loss: 1.9857  Acc@1: 62.5000 (59.9848)  Acc@5: 81.2500 (78.6204)  time: 1.9615  data: 0.0002  max mem: 6899
Train: Epoch[ 2/10]  [44/45]  eta: 0:00:01  Lr: 0.001219  Loss: 1.4638  Acc@1: 63.2653 (60.2792)  Acc@5: 81.2500 (78.9529)  time: 1.9486  data: 0.0002  max mem: 6899
Train: Epoch[ 2/10] Total time: 0:01:29 (1.9941 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.4638  Acc@1: 63.2653 (60.2792)  Acc@5: 81.2500 (78.9529)
Train: Epoch[ 3/10]  [ 0/45]  eta: 0:02:19  Lr: 0.001131  Loss: 1.2556  Acc@1: 62.5000 (62.5000)  Acc@5: 78.1250 (78.1250)  time: 3.0957  data: 0.7848  max mem: 6899
Train: Epoch[ 3/10]  [10/45]  eta: 0:01:12  Lr: 0.001131  Loss: 1.5734  Acc@1: 62.5000 (63.2102)  Acc@5: 79.6875 (82.5284)  time: 2.0801  data: 0.0716  max mem: 6899
Train: Epoch[ 3/10]  [20/45]  eta: 0:00:50  Lr: 0.001131  Loss: 1.4997  Acc@1: 64.0625 (63.9881)  Acc@5: 84.3750 (83.4821)  time: 1.9748  data: 0.0002  max mem: 6899
Train: Epoch[ 3/10]  [30/45]  eta: 0:00:30  Lr: 0.001131  Loss: 1.3993  Acc@1: 65.6250 (65.2218)  Acc@5: 84.3750 (84.0726)  time: 1.9741  data: 0.0002  max mem: 6899
Train: Epoch[ 3/10]  [40/45]  eta: 0:00:10  Lr: 0.001131  Loss: 1.2343  Acc@1: 67.1875 (65.6631)  Acc@5: 82.8125 (84.1082)  time: 1.9812  data: 0.0002  max mem: 6899
Train: Epoch[ 3/10]  [44/45]  eta: 0:00:01  Lr: 0.001131  Loss: 1.2936  Acc@1: 68.7500 (65.8639)  Acc@5: 82.8125 (83.8394)  time: 1.9659  data: 0.0002  max mem: 6899
Train: Epoch[ 3/10] Total time: 0:01:29 (1.9984 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.2936  Acc@1: 68.7500 (65.8639)  Acc@5: 82.8125 (83.8394)
Train: Epoch[ 4/10]  [ 0/45]  eta: 0:02:25  Lr: 0.000992  Loss: 1.1334  Acc@1: 73.4375 (73.4375)  Acc@5: 92.1875 (92.1875)  time: 3.2350  data: 0.8352  max mem: 6899
Train: Epoch[ 4/10]  [10/45]  eta: 0:01:12  Lr: 0.000992  Loss: 1.6076  Acc@1: 68.7500 (68.7500)  Acc@5: 85.9375 (86.2216)  time: 2.0837  data: 0.0762  max mem: 6899
Train: Epoch[ 4/10]  [20/45]  eta: 0:00:50  Lr: 0.000992  Loss: 1.2641  Acc@1: 68.7500 (68.8244)  Acc@5: 85.9375 (86.8304)  time: 1.9610  data: 0.0003  max mem: 6899
Train: Epoch[ 4/10]  [30/45]  eta: 0:00:29  Lr: 0.000992  Loss: 1.3532  Acc@1: 65.6250 (68.2460)  Acc@5: 85.9375 (87.0464)  time: 1.9529  data: 0.0003  max mem: 6899
Train: Epoch[ 4/10]  [40/45]  eta: 0:00:09  Lr: 0.000992  Loss: 1.1411  Acc@1: 67.1875 (68.6738)  Acc@5: 85.9375 (86.8902)  time: 1.9519  data: 0.0003  max mem: 6899
Train: Epoch[ 4/10]  [44/45]  eta: 0:00:01  Lr: 0.000992  Loss: 1.4933  Acc@1: 67.1875 (68.2723)  Acc@5: 84.3750 (86.7714)  time: 1.9400  data: 0.0002  max mem: 6899
Train: Epoch[ 4/10] Total time: 0:01:29 (1.9813 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.4933  Acc@1: 67.1875 (68.2723)  Acc@5: 84.3750 (86.7714)
Train: Epoch[ 5/10]  [ 0/45]  eta: 0:02:22  Lr: 0.000818  Loss: 1.3388  Acc@1: 78.1250 (78.1250)  Acc@5: 85.9375 (85.9375)  time: 3.1638  data: 0.8427  max mem: 6899
Train: Epoch[ 5/10]  [10/45]  eta: 0:01:12  Lr: 0.000818  Loss: 0.9622  Acc@1: 73.4375 (71.3068)  Acc@5: 87.5000 (86.5057)  time: 2.0682  data: 0.0768  max mem: 6899
Train: Epoch[ 5/10]  [20/45]  eta: 0:00:50  Lr: 0.000818  Loss: 1.1524  Acc@1: 71.8750 (71.5030)  Acc@5: 87.5000 (87.2024)  time: 1.9670  data: 0.0002  max mem: 6899
Train: Epoch[ 5/10]  [30/45]  eta: 0:00:30  Lr: 0.000818  Loss: 0.9104  Acc@1: 71.8750 (71.9254)  Acc@5: 89.0625 (87.7016)  time: 1.9708  data: 0.0002  max mem: 6899
Train: Epoch[ 5/10]  [40/45]  eta: 0:00:09  Lr: 0.000818  Loss: 1.2011  Acc@1: 71.8750 (71.6845)  Acc@5: 87.5000 (87.6143)  time: 1.9580  data: 0.0002  max mem: 6899
Train: Epoch[ 5/10]  [44/45]  eta: 0:00:01  Lr: 0.000818  Loss: 1.0805  Acc@1: 71.8750 (71.7627)  Acc@5: 87.5000 (87.6091)  time: 1.9430  data: 0.0002  max mem: 6899
Train: Epoch[ 5/10] Total time: 0:01:29 (1.9846 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.0805  Acc@1: 71.8750 (71.7627)  Acc@5: 87.5000 (87.6091)
Train: Epoch[ 6/10]  [ 0/45]  eta: 0:02:24  Lr: 0.000625  Loss: 1.0316  Acc@1: 71.8750 (71.8750)  Acc@5: 89.0625 (89.0625)  time: 3.2028  data: 0.8939  max mem: 6899
Train: Epoch[ 6/10]  [10/45]  eta: 0:01:12  Lr: 0.000625  Loss: 0.9241  Acc@1: 76.5625 (74.8580)  Acc@5: 89.0625 (89.9148)  time: 2.0722  data: 0.0815  max mem: 6899
Train: Epoch[ 6/10]  [20/45]  eta: 0:00:50  Lr: 0.000625  Loss: 0.9691  Acc@1: 75.0000 (75.0744)  Acc@5: 90.6250 (90.1786)  time: 1.9697  data: 0.0003  max mem: 6899
Train: Epoch[ 6/10]  [30/45]  eta: 0:00:30  Lr: 0.000625  Loss: 1.1602  Acc@1: 73.4375 (73.8407)  Acc@5: 89.0625 (89.5161)  time: 1.9644  data: 0.0002  max mem: 6899
Train: Epoch[ 6/10]  [40/45]  eta: 0:00:09  Lr: 0.000625  Loss: 0.9934  Acc@1: 71.8750 (73.8567)  Acc@5: 87.5000 (89.1006)  time: 1.9477  data: 0.0002  max mem: 6899
Train: Epoch[ 6/10]  [44/45]  eta: 0:00:01  Lr: 0.000625  Loss: 1.1273  Acc@1: 71.8750 (73.3333)  Acc@5: 87.5000 (88.6911)  time: 1.9347  data: 0.0002  max mem: 6899
Train: Epoch[ 6/10] Total time: 0:01:29 (1.9822 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.1273  Acc@1: 71.8750 (73.3333)  Acc@5: 87.5000 (88.6911)
Train: Epoch[ 7/10]  [ 0/45]  eta: 0:02:27  Lr: 0.000432  Loss: 0.6909  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 3.2704  data: 0.9165  max mem: 6899
Train: Epoch[ 7/10]  [10/45]  eta: 0:01:12  Lr: 0.000432  Loss: 0.8537  Acc@1: 70.3125 (72.7273)  Acc@5: 92.1875 (90.9091)  time: 2.0753  data: 0.0835  max mem: 6899
Train: Epoch[ 7/10]  [20/45]  eta: 0:00:50  Lr: 0.000432  Loss: 0.6688  Acc@1: 73.4375 (72.8423)  Acc@5: 90.6250 (90.3274)  time: 1.9643  data: 0.0002  max mem: 6899
Train: Epoch[ 7/10]  [30/45]  eta: 0:00:30  Lr: 0.000432  Loss: 0.8844  Acc@1: 73.4375 (72.2278)  Acc@5: 87.5000 (89.6169)  time: 1.9658  data: 0.0002  max mem: 6899
Train: Epoch[ 7/10]  [40/45]  eta: 0:00:09  Lr: 0.000432  Loss: 0.9210  Acc@1: 75.0000 (73.1326)  Acc@5: 89.0625 (89.5960)  time: 1.9570  data: 0.0002  max mem: 6899
Train: Epoch[ 7/10]  [44/45]  eta: 0:00:01  Lr: 0.000432  Loss: 0.8523  Acc@1: 75.0000 (73.1937)  Acc@5: 89.0625 (89.5986)  time: 1.9467  data: 0.0002  max mem: 6899
Train: Epoch[ 7/10] Total time: 0:01:29 (1.9878 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.8523  Acc@1: 75.0000 (73.1937)  Acc@5: 89.0625 (89.5986)
Train: Epoch[ 8/10]  [ 0/45]  eta: 0:02:27  Lr: 0.000258  Loss: 0.8652  Acc@1: 62.5000 (62.5000)  Acc@5: 89.0625 (89.0625)  time: 3.2822  data: 1.0323  max mem: 6899
Train: Epoch[ 8/10]  [10/45]  eta: 0:01:12  Lr: 0.000258  Loss: 0.6920  Acc@1: 75.0000 (74.2898)  Acc@5: 90.6250 (91.1932)  time: 2.0795  data: 0.0941  max mem: 6899
Train: Epoch[ 8/10]  [20/45]  eta: 0:00:50  Lr: 0.000258  Loss: 0.7979  Acc@1: 75.0000 (74.9256)  Acc@5: 90.6250 (90.6250)  time: 1.9539  data: 0.0002  max mem: 6899
Train: Epoch[ 8/10]  [30/45]  eta: 0:00:29  Lr: 0.000258  Loss: 0.7949  Acc@1: 78.1250 (75.2016)  Acc@5: 90.6250 (90.4234)  time: 1.9528  data: 0.0002  max mem: 6899
Train: Epoch[ 8/10]  [40/45]  eta: 0:00:09  Lr: 0.000258  Loss: 0.6234  Acc@1: 75.0000 (74.7713)  Acc@5: 90.6250 (90.1296)  time: 1.9657  data: 0.0002  max mem: 6899
Train: Epoch[ 8/10]  [44/45]  eta: 0:00:01  Lr: 0.000258  Loss: 0.5917  Acc@1: 75.0000 (75.0087)  Acc@5: 90.6250 (90.1571)  time: 1.9514  data: 0.0002  max mem: 6899
Train: Epoch[ 8/10] Total time: 0:01:29 (1.9848 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.5917  Acc@1: 75.0000 (75.0087)  Acc@5: 90.6250 (90.1571)
Train: Epoch[ 9/10]  [ 0/45]  eta: 0:02:21  Lr: 0.000119  Loss: 0.5377  Acc@1: 79.6875 (79.6875)  Acc@5: 93.7500 (93.7500)  time: 3.1344  data: 0.7528  max mem: 6899
Train: Epoch[ 9/10]  [10/45]  eta: 0:01:12  Lr: 0.000119  Loss: 0.6904  Acc@1: 71.8750 (72.0170)  Acc@5: 92.1875 (89.7727)  time: 2.0604  data: 0.0687  max mem: 6899
Train: Epoch[ 9/10]  [20/45]  eta: 0:00:50  Lr: 0.000119  Loss: 0.5687  Acc@1: 76.5625 (74.4792)  Acc@5: 89.0625 (90.4762)  time: 1.9551  data: 0.0002  max mem: 6899
Train: Epoch[ 9/10]  [30/45]  eta: 0:00:29  Lr: 0.000119  Loss: 0.4774  Acc@1: 78.1250 (74.8488)  Acc@5: 89.0625 (90.1714)  time: 1.9586  data: 0.0002  max mem: 6899
Train: Epoch[ 9/10]  [40/45]  eta: 0:00:09  Lr: 0.000119  Loss: 0.6677  Acc@1: 75.0000 (75.0000)  Acc@5: 90.6250 (90.3963)  time: 1.9674  data: 0.0002  max mem: 6899
Train: Epoch[ 9/10]  [44/45]  eta: 0:00:01  Lr: 0.000119  Loss: 0.3631  Acc@1: 75.0000 (74.9738)  Acc@5: 90.6250 (90.6806)  time: 1.9523  data: 0.0002  max mem: 6899
Train: Epoch[ 9/10] Total time: 0:01:29 (1.9833 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.3631  Acc@1: 75.0000 (74.9738)  Acc@5: 90.6250 (90.6806)
Train: Epoch[10/10]  [ 0/45]  eta: 0:02:23  Lr: 0.000031  Loss: 0.5382  Acc@1: 71.8750 (71.8750)  Acc@5: 89.0625 (89.0625)  time: 3.1783  data: 0.7847  max mem: 6899
Train: Epoch[10/10]  [10/45]  eta: 0:01:12  Lr: 0.000031  Loss: 0.3197  Acc@1: 73.4375 (73.7216)  Acc@5: 90.6250 (89.3466)  time: 2.0732  data: 0.0715  max mem: 6899
Train: Epoch[10/10]  [20/45]  eta: 0:00:50  Lr: 0.000031  Loss: 0.5335  Acc@1: 71.8750 (72.9911)  Acc@5: 89.0625 (89.3601)  time: 1.9583  data: 0.0002  max mem: 6899
Train: Epoch[10/10]  [30/45]  eta: 0:00:29  Lr: 0.000031  Loss: 0.4386  Acc@1: 71.8750 (73.2863)  Acc@5: 89.0625 (89.1129)  time: 1.9507  data: 0.0002  max mem: 6899
Train: Epoch[10/10]  [40/45]  eta: 0:00:09  Lr: 0.000031  Loss: 0.3743  Acc@1: 73.4375 (73.8567)  Acc@5: 89.0625 (89.5579)  time: 1.9470  data: 0.0002  max mem: 6899
Train: Epoch[10/10]  [44/45]  eta: 0:00:01  Lr: 0.000031  Loss: 0.3449  Acc@1: 73.4694 (74.2059)  Acc@5: 89.0625 (89.7731)  time: 1.9337  data: 0.0002  max mem: 6899
Train: Epoch[10/10] Total time: 0:01:28 (1.9760 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.3449  Acc@1: 73.4694 (74.2059)  Acc@5: 89.0625 (89.7731)
Test: [Task 1]  [ 0/11]  eta: 0:00:18  Loss: 0.4421 (0.4421)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 1.6405  data: 0.8211  max mem: 6899
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.6184 (0.6180)  Acc@1: 84.3750 (85.0291)  Acc@5: 96.8750 (95.4942)  time: 0.7036  data: 0.0751  max mem: 6899
Test: [Task 1] Total time: 0:00:07 (0.7126 s / it)
* Acc@1 85.029 Acc@5 95.494 loss 0.618
Batchwise eval time for task 1 = 0.7126457040960138
Test: [Task 2]  [ 0/12]  eta: 0:00:20  Loss: 0.2936 (0.2936)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.7079  data: 0.9765  max mem: 6899
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.6342 (0.6976)  Acc@1: 85.9375 (83.3807)  Acc@5: 95.3125 (94.0341)  time: 0.7194  data: 0.0892  max mem: 6899
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.6057 (0.6398)  Acc@1: 85.9375 (83.4043)  Acc@5: 95.3125 (94.0426)  time: 0.6743  data: 0.0817  max mem: 6899
Test: [Task 2] Total time: 0:00:08 (0.6809 s / it)
* Acc@1 83.404 Acc@5 94.043 loss 0.640
Batchwise eval time for task 2 = 0.6808927456537882
Test: [Task 3]  [0/7]  eta: 0:00:11  Loss: 1.0820 (1.0820)  Acc@1: 71.8750 (71.8750)  Acc@5: 89.0625 (89.0625)  time: 1.6470  data: 1.0031  max mem: 6899
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 0.9271 (1.0629)  Acc@1: 78.1250 (74.1935)  Acc@5: 92.1875 (92.6267)  time: 0.7533  data: 0.1435  max mem: 6899
Test: [Task 3] Total time: 0:00:05 (0.7655 s / it)
* Acc@1 74.194 Acc@5 92.627 loss 1.063
Batchwise eval time for task 3 = 0.7655191421508789
Test: [Task 4]  [ 0/10]  eta: 0:00:16  Loss: 0.9672 (0.9672)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.1875 (92.1875)  time: 1.6886  data: 0.8814  max mem: 6899
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 0.9495 (0.9210)  Acc@1: 75.0000 (76.6831)  Acc@5: 92.1875 (92.7750)  time: 0.6999  data: 0.0884  max mem: 6899
Test: [Task 4] Total time: 0:00:07 (0.7083 s / it)
* Acc@1 76.683 Acc@5 92.775 loss 0.921
Batchwise eval time for task 4 = 0.7083130359649659
Test: [Task 5]  [ 0/11]  eta: 0:00:19  Loss: 0.7908 (0.7908)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 1.8070  data: 1.0383  max mem: 6899
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 0.7908 (0.8606)  Acc@1: 81.2500 (78.8991)  Acc@5: 92.8571 (91.7431)  time: 0.7217  data: 0.0946  max mem: 6899
Test: [Task 5] Total time: 0:00:08 (0.7294 s / it)
* Acc@1 78.899 Acc@5 91.743 loss 0.861
Batchwise eval time for task 5 = 0.729370507326993
[Average accuracy till task5]	Acc@1: 80.1942	Acc@5: 93.3363	Loss: 0.8204	Forgetting: 2.5116	Backward: -2.4052
Eval time for task 5 = 36.65063953399658
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.8656)  Task:  5
Old Num K:  9 New Num K:  10
Task number:  5
Train: Epoch[ 1/10]  [ 0/35]  eta: 0:01:54  Lr: 0.001250  Loss: 3.1502  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 3.2603  data: 0.7199  max mem: 6938
Train: Epoch[ 1/10]  [10/35]  eta: 0:00:54  Lr: 0.001250  Loss: 2.5922  Acc@1: 1.5625 (2.6989)  Acc@5: 6.2500 (10.3693)  time: 2.1874  data: 0.0657  max mem: 6948
Train: Epoch[ 1/10]  [20/35]  eta: 0:00:32  Lr: 0.001250  Loss: 2.8112  Acc@1: 6.2500 (11.3839)  Acc@5: 28.1250 (26.7857)  time: 2.0900  data: 0.0002  max mem: 6948
Train: Epoch[ 1/10]  [30/35]  eta: 0:00:10  Lr: 0.001250  Loss: 1.6734  Acc@1: 29.6875 (19.9597)  Acc@5: 57.8125 (39.3649)  time: 2.0874  data: 0.0002  max mem: 6948
Train: Epoch[ 1/10]  [34/35]  eta: 0:00:02  Lr: 0.001250  Loss: 1.4871  Acc@1: 35.9375 (22.1721)  Acc@5: 60.9375 (42.0009)  time: 2.0869  data: 0.0002  max mem: 6948
Train: Epoch[ 1/10] Total time: 0:01:14 (2.1256 s / it)
Averaged stats: Lr: 0.001250  Loss: 1.4871  Acc@1: 35.9375 (22.1721)  Acc@5: 60.9375 (42.0009)
Train: Epoch[ 2/10]  [ 0/35]  eta: 0:01:51  Lr: 0.001219  Loss: 1.7593  Acc@1: 50.0000 (50.0000)  Acc@5: 79.6875 (79.6875)  time: 3.1729  data: 0.7481  max mem: 6948
Train: Epoch[ 2/10]  [10/35]  eta: 0:00:54  Lr: 0.001219  Loss: 1.9690  Acc@1: 51.5625 (54.6875)  Acc@5: 76.5625 (76.1364)  time: 2.1784  data: 0.0682  max mem: 6948
Train: Epoch[ 2/10]  [20/35]  eta: 0:00:31  Lr: 0.001219  Loss: 1.3043  Acc@1: 51.5625 (54.0923)  Acc@5: 75.0000 (75.8185)  time: 2.0780  data: 0.0002  max mem: 6948
Train: Epoch[ 2/10]  [30/35]  eta: 0:00:10  Lr: 0.001219  Loss: 1.7215  Acc@1: 54.6875 (55.3427)  Acc@5: 75.0000 (76.4113)  time: 2.0756  data: 0.0002  max mem: 6948
Train: Epoch[ 2/10]  [34/35]  eta: 0:00:02  Lr: 0.001219  Loss: 1.8575  Acc@1: 57.8125 (55.3402)  Acc@5: 76.5625 (76.8364)  time: 2.0446  data: 0.0002  max mem: 6948
Train: Epoch[ 2/10] Total time: 0:01:13 (2.0938 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.8575  Acc@1: 57.8125 (55.3402)  Acc@5: 76.5625 (76.8364)
Train: Epoch[ 3/10]  [ 0/35]  eta: 0:01:52  Lr: 0.001131  Loss: 1.3387  Acc@1: 59.3750 (59.3750)  Acc@5: 84.3750 (84.3750)  time: 3.2014  data: 0.7031  max mem: 6948
Train: Epoch[ 3/10]  [10/35]  eta: 0:00:54  Lr: 0.001131  Loss: 1.5979  Acc@1: 59.3750 (58.3807)  Acc@5: 81.2500 (79.6875)  time: 2.1919  data: 0.0641  max mem: 6948
Train: Epoch[ 3/10]  [20/35]  eta: 0:00:32  Lr: 0.001131  Loss: 0.9325  Acc@1: 60.9375 (61.9792)  Acc@5: 81.2500 (81.6220)  time: 2.0826  data: 0.0002  max mem: 6948
Train: Epoch[ 3/10]  [30/35]  eta: 0:00:10  Lr: 0.001131  Loss: 1.7769  Acc@1: 60.9375 (61.5423)  Acc@5: 85.9375 (82.3589)  time: 2.0729  data: 0.0002  max mem: 6948
Train: Epoch[ 3/10]  [34/35]  eta: 0:00:02  Lr: 0.001131  Loss: 1.3914  Acc@1: 62.5000 (62.0550)  Acc@5: 84.3750 (82.2893)  time: 2.0453  data: 0.0002  max mem: 6948
Train: Epoch[ 3/10] Total time: 0:01:13 (2.0972 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.3914  Acc@1: 62.5000 (62.0550)  Acc@5: 84.3750 (82.2893)
Train: Epoch[ 4/10]  [ 0/35]  eta: 0:01:50  Lr: 0.000992  Loss: 1.4331  Acc@1: 60.9375 (60.9375)  Acc@5: 81.2500 (81.2500)  time: 3.1566  data: 0.7490  max mem: 6948
Train: Epoch[ 4/10]  [10/35]  eta: 0:00:54  Lr: 0.000992  Loss: 1.1779  Acc@1: 64.0625 (64.3466)  Acc@5: 82.8125 (83.6648)  time: 2.1732  data: 0.0683  max mem: 6948
Train: Epoch[ 4/10]  [20/35]  eta: 0:00:31  Lr: 0.000992  Loss: 1.4001  Acc@1: 65.6250 (64.9554)  Acc@5: 85.9375 (84.8214)  time: 2.0776  data: 0.0002  max mem: 6948
Train: Epoch[ 4/10]  [30/35]  eta: 0:00:10  Lr: 0.000992  Loss: 1.0378  Acc@1: 65.6250 (65.2218)  Acc@5: 84.3750 (83.9214)  time: 2.0794  data: 0.0002  max mem: 6948
Train: Epoch[ 4/10]  [34/35]  eta: 0:00:02  Lr: 0.000992  Loss: 1.4493  Acc@1: 68.7500 (65.8855)  Acc@5: 84.3750 (84.0469)  time: 2.0483  data: 0.0002  max mem: 6948
Train: Epoch[ 4/10] Total time: 0:01:13 (2.0945 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.4493  Acc@1: 68.7500 (65.8855)  Acc@5: 84.3750 (84.0469)
Train: Epoch[ 5/10]  [ 0/35]  eta: 0:01:51  Lr: 0.000818  Loss: 1.4031  Acc@1: 67.1875 (67.1875)  Acc@5: 84.3750 (84.3750)  time: 3.1990  data: 0.7889  max mem: 6948
Train: Epoch[ 5/10]  [10/35]  eta: 0:00:55  Lr: 0.000818  Loss: 0.9840  Acc@1: 68.7500 (69.6023)  Acc@5: 87.5000 (87.2159)  time: 2.2074  data: 0.0719  max mem: 6948
Train: Epoch[ 5/10]  [20/35]  eta: 0:00:32  Lr: 0.000818  Loss: 1.0087  Acc@1: 68.7500 (69.8661)  Acc@5: 87.5000 (87.5000)  time: 2.0946  data: 0.0002  max mem: 6948
Train: Epoch[ 5/10]  [30/35]  eta: 0:00:10  Lr: 0.000818  Loss: 1.1100  Acc@1: 71.8750 (70.5141)  Acc@5: 87.5000 (87.9032)  time: 2.0772  data: 0.0002  max mem: 6948
Train: Epoch[ 5/10]  [34/35]  eta: 0:00:02  Lr: 0.000818  Loss: 0.7559  Acc@1: 70.3125 (70.3470)  Acc@5: 87.5000 (87.8774)  time: 2.0467  data: 0.0002  max mem: 6948
Train: Epoch[ 5/10] Total time: 0:01:13 (2.1040 s / it)
Averaged stats: Lr: 0.000818  Loss: 0.7559  Acc@1: 70.3125 (70.3470)  Acc@5: 87.5000 (87.8774)
Train: Epoch[ 6/10]  [ 0/35]  eta: 0:01:54  Lr: 0.000625  Loss: 1.0409  Acc@1: 65.6250 (65.6250)  Acc@5: 81.2500 (81.2500)  time: 3.2716  data: 0.7268  max mem: 6948
Train: Epoch[ 6/10]  [10/35]  eta: 0:00:54  Lr: 0.000625  Loss: 1.1289  Acc@1: 68.7500 (70.3125)  Acc@5: 85.9375 (85.9375)  time: 2.1842  data: 0.0663  max mem: 6948
Train: Epoch[ 6/10]  [20/35]  eta: 0:00:31  Lr: 0.000625  Loss: 1.0804  Acc@1: 68.7500 (70.6101)  Acc@5: 85.9375 (86.2351)  time: 2.0750  data: 0.0002  max mem: 6948
Train: Epoch[ 6/10]  [30/35]  eta: 0:00:10  Lr: 0.000625  Loss: 0.9394  Acc@1: 71.8750 (71.4718)  Acc@5: 87.5000 (87.1472)  time: 2.0847  data: 0.0002  max mem: 6948
Train: Epoch[ 6/10]  [34/35]  eta: 0:00:02  Lr: 0.000625  Loss: 0.9857  Acc@1: 71.8750 (71.4736)  Acc@5: 87.5000 (86.8409)  time: 2.0559  data: 0.0002  max mem: 6948
Train: Epoch[ 6/10] Total time: 0:01:13 (2.1015 s / it)
Averaged stats: Lr: 0.000625  Loss: 0.9857  Acc@1: 71.8750 (71.4736)  Acc@5: 87.5000 (86.8409)
Train: Epoch[ 7/10]  [ 0/35]  eta: 0:01:55  Lr: 0.000432  Loss: 0.7649  Acc@1: 73.4375 (73.4375)  Acc@5: 90.6250 (90.6250)  time: 3.2964  data: 0.9839  max mem: 6948
Train: Epoch[ 7/10]  [10/35]  eta: 0:00:54  Lr: 0.000432  Loss: 0.7172  Acc@1: 75.0000 (75.4261)  Acc@5: 89.0625 (90.0568)  time: 2.1905  data: 0.0896  max mem: 6948
Train: Epoch[ 7/10]  [20/35]  eta: 0:00:32  Lr: 0.000432  Loss: 1.1433  Acc@1: 75.0000 (72.4702)  Acc@5: 89.0625 (89.3601)  time: 2.0754  data: 0.0002  max mem: 6948
Train: Epoch[ 7/10]  [30/35]  eta: 0:00:10  Lr: 0.000432  Loss: 0.6777  Acc@1: 71.8750 (72.5806)  Acc@5: 87.5000 (88.9113)  time: 2.0707  data: 0.0003  max mem: 6948
Train: Epoch[ 7/10]  [34/35]  eta: 0:00:02  Lr: 0.000432  Loss: 0.6967  Acc@1: 71.8750 (72.9157)  Acc@5: 87.5000 (88.9590)  time: 2.0521  data: 0.0002  max mem: 6948
Train: Epoch[ 7/10] Total time: 0:01:13 (2.1010 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.6967  Acc@1: 71.8750 (72.9157)  Acc@5: 87.5000 (88.9590)
Train: Epoch[ 8/10]  [ 0/35]  eta: 0:01:50  Lr: 0.000258  Loss: 0.6649  Acc@1: 78.1250 (78.1250)  Acc@5: 95.3125 (95.3125)  time: 3.1653  data: 0.7269  max mem: 6948
Train: Epoch[ 8/10]  [10/35]  eta: 0:00:54  Lr: 0.000258  Loss: 0.7210  Acc@1: 70.3125 (72.1591)  Acc@5: 89.0625 (89.4886)  time: 2.1757  data: 0.0663  max mem: 6948
Train: Epoch[ 8/10]  [20/35]  eta: 0:00:31  Lr: 0.000258  Loss: 0.5126  Acc@1: 71.8750 (72.7679)  Acc@5: 87.5000 (88.6161)  time: 2.0766  data: 0.0002  max mem: 6948
Train: Epoch[ 8/10]  [30/35]  eta: 0:00:10  Lr: 0.000258  Loss: 0.6623  Acc@1: 73.4375 (72.8327)  Acc@5: 89.0625 (88.9617)  time: 2.0764  data: 0.0002  max mem: 6948
Train: Epoch[ 8/10]  [34/35]  eta: 0:00:02  Lr: 0.000258  Loss: 0.7742  Acc@1: 73.4375 (72.9608)  Acc@5: 89.0625 (88.9139)  time: 2.0486  data: 0.0002  max mem: 6948
Train: Epoch[ 8/10] Total time: 0:01:13 (2.0950 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.7742  Acc@1: 73.4375 (72.9608)  Acc@5: 89.0625 (88.9139)
Train: Epoch[ 9/10]  [ 0/35]  eta: 0:01:50  Lr: 0.000119  Loss: 0.6351  Acc@1: 76.5625 (76.5625)  Acc@5: 90.6250 (90.6250)  time: 3.1484  data: 0.7774  max mem: 6948
Train: Epoch[ 9/10]  [10/35]  eta: 0:00:55  Lr: 0.000119  Loss: 0.3514  Acc@1: 73.4375 (74.1477)  Acc@5: 89.0625 (88.3523)  time: 2.2026  data: 0.0709  max mem: 6948
Train: Epoch[ 9/10]  [20/35]  eta: 0:00:32  Lr: 0.000119  Loss: 0.5475  Acc@1: 73.4375 (74.3304)  Acc@5: 87.5000 (87.6488)  time: 2.0972  data: 0.0002  max mem: 6948
Train: Epoch[ 9/10]  [30/35]  eta: 0:00:10  Lr: 0.000119  Loss: 0.7670  Acc@1: 73.4375 (73.6895)  Acc@5: 87.5000 (87.9032)  time: 2.0829  data: 0.0002  max mem: 6948
Train: Epoch[ 9/10]  [34/35]  eta: 0:00:02  Lr: 0.000119  Loss: 0.3492  Acc@1: 76.5625 (73.7720)  Acc@5: 89.0625 (88.4633)  time: 2.0536  data: 0.0002  max mem: 6948
Train: Epoch[ 9/10] Total time: 0:01:13 (2.1069 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.3492  Acc@1: 76.5625 (73.7720)  Acc@5: 89.0625 (88.4633)
Train: Epoch[10/10]  [ 0/35]  eta: 0:01:53  Lr: 0.000031  Loss: 0.5490  Acc@1: 76.5625 (76.5625)  Acc@5: 90.6250 (90.6250)  time: 3.2554  data: 0.8471  max mem: 6948
Train: Epoch[10/10]  [10/35]  eta: 0:00:54  Lr: 0.000031  Loss: 0.5405  Acc@1: 76.5625 (75.9943)  Acc@5: 90.6250 (89.6307)  time: 2.1915  data: 0.0772  max mem: 6948
Train: Epoch[10/10]  [20/35]  eta: 0:00:32  Lr: 0.000031  Loss: 0.5185  Acc@1: 75.0000 (75.4464)  Acc@5: 89.0625 (89.6577)  time: 2.0844  data: 0.0002  max mem: 6948
Train: Epoch[10/10]  [30/35]  eta: 0:00:10  Lr: 0.000031  Loss: 0.3499  Acc@1: 75.0000 (75.3528)  Acc@5: 89.0625 (89.6169)  time: 2.0963  data: 0.0002  max mem: 6948
Train: Epoch[10/10]  [34/35]  eta: 0:00:02  Lr: 0.000031  Loss: 0.5273  Acc@1: 73.4375 (75.1239)  Acc@5: 89.0625 (89.5448)  time: 2.0688  data: 0.0002  max mem: 6948
Train: Epoch[10/10] Total time: 0:01:13 (2.1116 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.5273  Acc@1: 73.4375 (75.1239)  Acc@5: 89.0625 (89.5448)
Test: [Task 1]  [ 0/11]  eta: 0:00:17  Loss: 0.5383 (0.5383)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 1.6010  data: 0.8299  max mem: 6948
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.6781 (0.6815)  Acc@1: 82.8125 (83.4302)  Acc@5: 95.3125 (94.4767)  time: 0.7306  data: 0.0757  max mem: 6948
Test: [Task 1] Total time: 0:00:08 (0.7383 s / it)
* Acc@1 83.430 Acc@5 94.477 loss 0.681
Batchwise eval time for task 1 = 0.7383366064591841
Test: [Task 2]  [ 0/12]  eta: 0:00:21  Loss: 0.3110 (0.3110)  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 1.8099  data: 0.9625  max mem: 6948
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.6703 (0.7411)  Acc@1: 84.3750 (82.1023)  Acc@5: 95.3125 (93.0398)  time: 0.7492  data: 0.0878  max mem: 6948
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.6072 (0.6797)  Acc@1: 84.3750 (82.1277)  Acc@5: 95.3125 (93.0496)  time: 0.7027  data: 0.0805  max mem: 6948
Test: [Task 2] Total time: 0:00:08 (0.7097 s / it)
* Acc@1 82.128 Acc@5 93.050 loss 0.680
Batchwise eval time for task 2 = 0.7097493410110474
Test: [Task 3]  [0/7]  eta: 0:00:12  Loss: 1.1938 (1.1938)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 1.7246  data: 1.0181  max mem: 6948
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 0.9629 (1.1143)  Acc@1: 76.5625 (73.2719)  Acc@5: 92.1875 (91.4747)  time: 0.7856  data: 0.1457  max mem: 6948
Test: [Task 3] Total time: 0:00:05 (0.7977 s / it)
* Acc@1 73.272 Acc@5 91.475 loss 1.114
Batchwise eval time for task 3 = 0.7977635179247174
Test: [Task 4]  [ 0/10]  eta: 0:00:16  Loss: 1.0083 (1.0083)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 1.6420  data: 0.8698  max mem: 6948
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 0.9846 (0.9801)  Acc@1: 75.0000 (77.0115)  Acc@5: 92.1875 (91.9540)  time: 0.7214  data: 0.0878  max mem: 6948
Test: [Task 4] Total time: 0:00:07 (0.7296 s / it)
* Acc@1 77.011 Acc@5 91.954 loss 0.980
Batchwise eval time for task 4 = 0.7296220064163208
Test: [Task 5]  [ 0/11]  eta: 0:00:19  Loss: 0.8059 (0.8059)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 1.7490  data: 0.9631  max mem: 6948
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 0.8915 (0.9190)  Acc@1: 79.6875 (78.5933)  Acc@5: 90.6250 (90.9786)  time: 0.7099  data: 0.0878  max mem: 6948
Test: [Task 5] Total time: 0:00:07 (0.7187 s / it)
* Acc@1 78.593 Acc@5 90.979 loss 0.919
Batchwise eval time for task 5 = 0.7187212597240101
Test: [Task 6]  [0/9]  eta: 0:00:15  Loss: 1.3452 (1.3452)  Acc@1: 70.3125 (70.3125)  Acc@5: 89.0625 (89.0625)  time: 1.7496  data: 0.9159  max mem: 6948
Test: [Task 6]  [8/9]  eta: 0:00:00  Loss: 0.8395 (0.8788)  Acc@1: 82.8125 (79.6733)  Acc@5: 92.1875 (93.1034)  time: 0.7931  data: 0.1020  max mem: 6948
Test: [Task 6] Total time: 0:00:07 (0.8043 s / it)
* Acc@1 79.673 Acc@5 93.103 loss 0.879
Batchwise eval time for task 6 = 0.8043875164455838
[Average accuracy till task6]	Acc@1: 79.4562	Acc@5: 92.5062	Loss: 0.8756	Forgetting: 2.7642	Backward: -2.6791
Eval time for task 6 = 44.86616063117981
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.7597)  Task:  6
Old Num K:  10 New Num K:  12
Task number:  6
Train: Epoch[ 1/10]  [ 0/36]  eta: 0:02:16  Lr: 0.001250  Loss: 3.0561  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 3.7989  data: 0.8716  max mem: 7042
Train: Epoch[ 1/10]  [10/36]  eta: 0:01:05  Lr: 0.001250  Loss: 2.7822  Acc@1: 0.0000 (1.4205)  Acc@5: 7.8125 (9.9432)  time: 2.5148  data: 0.0794  max mem: 7055
Train: Epoch[ 1/10]  [20/36]  eta: 0:00:39  Lr: 0.001250  Loss: 3.1644  Acc@1: 4.6875 (9.0774)  Acc@5: 25.0000 (23.6607)  time: 2.3829  data: 0.0002  max mem: 7055
Train: Epoch[ 1/10]  [30/36]  eta: 0:00:14  Lr: 0.001250  Loss: 2.2659  Acc@1: 23.4375 (15.6754)  Acc@5: 48.4375 (33.5181)  time: 2.3892  data: 0.0002  max mem: 7055
Train: Epoch[ 1/10]  [35/36]  eta: 0:00:02  Lr: 0.001250  Loss: 2.4503  Acc@1: 28.1250 (18.0702)  Acc@5: 53.1250 (36.3158)  time: 2.3841  data: 0.0002  max mem: 7055
Train: Epoch[ 1/10] Total time: 0:01:27 (2.4261 s / it)
Averaged stats: Lr: 0.001250  Loss: 2.4503  Acc@1: 28.1250 (18.0702)  Acc@5: 53.1250 (36.3158)
Train: Epoch[ 2/10]  [ 0/36]  eta: 0:02:08  Lr: 0.001219  Loss: 2.4209  Acc@1: 43.7500 (43.7500)  Acc@5: 67.1875 (67.1875)  time: 3.5822  data: 0.6998  max mem: 7055
Train: Epoch[ 2/10]  [10/36]  eta: 0:01:04  Lr: 0.001219  Loss: 1.6163  Acc@1: 45.3125 (47.8693)  Acc@5: 70.3125 (69.1761)  time: 2.4855  data: 0.0638  max mem: 7055
Train: Epoch[ 2/10]  [20/36]  eta: 0:00:38  Lr: 0.001219  Loss: 2.1919  Acc@1: 48.4375 (49.4048)  Acc@5: 70.3125 (69.7917)  time: 2.3732  data: 0.0002  max mem: 7055
Train: Epoch[ 2/10]  [30/36]  eta: 0:00:14  Lr: 0.001219  Loss: 1.3837  Acc@1: 54.6875 (51.1593)  Acc@5: 71.8750 (70.2117)  time: 2.3799  data: 0.0002  max mem: 7055
Train: Epoch[ 2/10]  [35/36]  eta: 0:00:02  Lr: 0.001219  Loss: 1.5994  Acc@1: 52.5000 (51.7982)  Acc@5: 68.7500 (70.6140)  time: 2.3376  data: 0.0002  max mem: 7055
Train: Epoch[ 2/10] Total time: 0:01:26 (2.3922 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.5994  Acc@1: 52.5000 (51.7982)  Acc@5: 68.7500 (70.6140)
Train: Epoch[ 3/10]  [ 0/36]  eta: 0:02:06  Lr: 0.001131  Loss: 1.9331  Acc@1: 51.5625 (51.5625)  Acc@5: 73.4375 (73.4375)  time: 3.5066  data: 0.7549  max mem: 7055
Train: Epoch[ 3/10]  [10/36]  eta: 0:01:04  Lr: 0.001131  Loss: 1.3010  Acc@1: 59.3750 (59.3750)  Acc@5: 76.5625 (76.5625)  time: 2.4683  data: 0.0688  max mem: 7055
Train: Epoch[ 3/10]  [20/36]  eta: 0:00:38  Lr: 0.001131  Loss: 1.8217  Acc@1: 62.5000 (59.8214)  Acc@5: 76.5625 (76.2649)  time: 2.3762  data: 0.0002  max mem: 7055
Train: Epoch[ 3/10]  [30/36]  eta: 0:00:14  Lr: 0.001131  Loss: 1.3493  Acc@1: 60.9375 (59.6774)  Acc@5: 76.5625 (76.5625)  time: 2.3826  data: 0.0002  max mem: 7055
Train: Epoch[ 3/10]  [35/36]  eta: 0:00:02  Lr: 0.001131  Loss: 1.6003  Acc@1: 60.0000 (60.2193)  Acc@5: 76.5625 (77.1930)  time: 2.3459  data: 0.0002  max mem: 7055
Train: Epoch[ 3/10] Total time: 0:01:26 (2.3899 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.6003  Acc@1: 60.0000 (60.2193)  Acc@5: 76.5625 (77.1930)
Train: Epoch[ 4/10]  [ 0/36]  eta: 0:02:05  Lr: 0.000992  Loss: 1.7154  Acc@1: 59.3750 (59.3750)  Acc@5: 84.3750 (84.3750)  time: 3.4883  data: 0.7647  max mem: 7055
Train: Epoch[ 4/10]  [10/36]  eta: 0:01:04  Lr: 0.000992  Loss: 1.4104  Acc@1: 59.3750 (58.9489)  Acc@5: 79.6875 (80.8239)  time: 2.4805  data: 0.0698  max mem: 7055
Train: Epoch[ 4/10]  [20/36]  eta: 0:00:39  Lr: 0.000992  Loss: 1.3709  Acc@1: 59.3750 (60.7143)  Acc@5: 79.6875 (81.3988)  time: 2.3958  data: 0.0003  max mem: 7055
Train: Epoch[ 4/10]  [30/36]  eta: 0:00:14  Lr: 0.000992  Loss: 1.1388  Acc@1: 64.0625 (61.6935)  Acc@5: 81.2500 (80.8972)  time: 2.3975  data: 0.0003  max mem: 7055
Train: Epoch[ 4/10]  [35/36]  eta: 0:00:02  Lr: 0.000992  Loss: 1.2081  Acc@1: 64.0625 (62.3684)  Acc@5: 81.2500 (81.0088)  time: 2.3586  data: 0.0002  max mem: 7055
Train: Epoch[ 4/10] Total time: 0:01:26 (2.4033 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.2081  Acc@1: 64.0625 (62.3684)  Acc@5: 81.2500 (81.0088)
Train: Epoch[ 5/10]  [ 0/36]  eta: 0:02:08  Lr: 0.000818  Loss: 1.2621  Acc@1: 73.4375 (73.4375)  Acc@5: 84.3750 (84.3750)  time: 3.5753  data: 0.7325  max mem: 7055
Train: Epoch[ 5/10]  [10/36]  eta: 0:01:04  Lr: 0.000818  Loss: 1.4462  Acc@1: 67.1875 (67.1875)  Acc@5: 81.2500 (79.8295)  time: 2.4959  data: 0.0668  max mem: 7055
Train: Epoch[ 5/10]  [20/36]  eta: 0:00:39  Lr: 0.000818  Loss: 1.2131  Acc@1: 68.7500 (68.5268)  Acc@5: 81.2500 (81.7708)  time: 2.3975  data: 0.0002  max mem: 7055
Train: Epoch[ 5/10]  [30/36]  eta: 0:00:14  Lr: 0.000818  Loss: 0.9830  Acc@1: 68.7500 (68.5484)  Acc@5: 84.3750 (83.3669)  time: 2.3962  data: 0.0002  max mem: 7055
Train: Epoch[ 5/10]  [35/36]  eta: 0:00:02  Lr: 0.000818  Loss: 1.1040  Acc@1: 68.7500 (67.9825)  Acc@5: 84.3750 (82.9825)  time: 2.3478  data: 0.0002  max mem: 7055
Train: Epoch[ 5/10] Total time: 0:01:26 (2.4078 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.1040  Acc@1: 68.7500 (67.9825)  Acc@5: 84.3750 (82.9825)
Train: Epoch[ 6/10]  [ 0/36]  eta: 0:02:15  Lr: 0.000625  Loss: 1.3534  Acc@1: 57.8125 (57.8125)  Acc@5: 78.1250 (78.1250)  time: 3.7672  data: 0.8296  max mem: 7055
Train: Epoch[ 6/10]  [10/36]  eta: 0:01:05  Lr: 0.000625  Loss: 0.9621  Acc@1: 70.3125 (69.1761)  Acc@5: 84.3750 (84.0909)  time: 2.5047  data: 0.0756  max mem: 7055
Train: Epoch[ 6/10]  [20/36]  eta: 0:00:39  Lr: 0.000625  Loss: 1.0849  Acc@1: 70.3125 (69.1220)  Acc@5: 84.3750 (84.3750)  time: 2.3903  data: 0.0002  max mem: 7055
Train: Epoch[ 6/10]  [30/36]  eta: 0:00:14  Lr: 0.000625  Loss: 1.0577  Acc@1: 68.7500 (69.2036)  Acc@5: 84.3750 (84.2238)  time: 2.3902  data: 0.0002  max mem: 7055
Train: Epoch[ 6/10]  [35/36]  eta: 0:00:02  Lr: 0.000625  Loss: 1.1562  Acc@1: 67.1875 (69.4737)  Acc@5: 85.9375 (84.0351)  time: 2.3411  data: 0.0002  max mem: 7055
Train: Epoch[ 6/10] Total time: 0:01:26 (2.4055 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.1562  Acc@1: 67.1875 (69.4737)  Acc@5: 85.9375 (84.0351)
Train: Epoch[ 7/10]  [ 0/36]  eta: 0:02:09  Lr: 0.000432  Loss: 0.9971  Acc@1: 75.0000 (75.0000)  Acc@5: 90.6250 (90.6250)  time: 3.5912  data: 0.7636  max mem: 7055
Train: Epoch[ 7/10]  [10/36]  eta: 0:01:04  Lr: 0.000432  Loss: 0.8039  Acc@1: 75.0000 (71.8750)  Acc@5: 87.5000 (88.0682)  time: 2.4984  data: 0.0696  max mem: 7055
Train: Epoch[ 7/10]  [20/36]  eta: 0:00:39  Lr: 0.000432  Loss: 1.0412  Acc@1: 71.8750 (72.2470)  Acc@5: 87.5000 (88.0208)  time: 2.4012  data: 0.0002  max mem: 7055
Train: Epoch[ 7/10]  [30/36]  eta: 0:00:14  Lr: 0.000432  Loss: 0.8793  Acc@1: 71.8750 (71.3710)  Acc@5: 87.5000 (87.4496)  time: 2.3992  data: 0.0002  max mem: 7055
Train: Epoch[ 7/10]  [35/36]  eta: 0:00:02  Lr: 0.000432  Loss: 0.7586  Acc@1: 71.8750 (71.2719)  Acc@5: 87.5000 (87.5000)  time: 2.3488  data: 0.0002  max mem: 7055
Train: Epoch[ 7/10] Total time: 0:01:26 (2.4088 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.7586  Acc@1: 71.8750 (71.2719)  Acc@5: 87.5000 (87.5000)
Train: Epoch[ 8/10]  [ 0/36]  eta: 0:02:11  Lr: 0.000258  Loss: 0.8852  Acc@1: 78.1250 (78.1250)  Acc@5: 89.0625 (89.0625)  time: 3.6612  data: 0.8886  max mem: 7055
Train: Epoch[ 8/10]  [10/36]  eta: 0:01:05  Lr: 0.000258  Loss: 0.6426  Acc@1: 68.7500 (69.0341)  Acc@5: 85.9375 (85.3693)  time: 2.5303  data: 0.0810  max mem: 7055
Train: Epoch[ 8/10]  [20/36]  eta: 0:00:39  Lr: 0.000258  Loss: 0.7170  Acc@1: 68.7500 (70.1637)  Acc@5: 85.9375 (85.8631)  time: 2.4052  data: 0.0002  max mem: 7055
Train: Epoch[ 8/10]  [30/36]  eta: 0:00:14  Lr: 0.000258  Loss: 0.6353  Acc@1: 70.3125 (70.4637)  Acc@5: 87.5000 (85.9879)  time: 2.3929  data: 0.0002  max mem: 7055
Train: Epoch[ 8/10]  [35/36]  eta: 0:00:02  Lr: 0.000258  Loss: 0.6424  Acc@1: 70.3125 (70.5263)  Acc@5: 85.9375 (85.9649)  time: 2.3512  data: 0.0002  max mem: 7055
Train: Epoch[ 8/10] Total time: 0:01:26 (2.4157 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.6424  Acc@1: 70.3125 (70.5263)  Acc@5: 85.9375 (85.9649)
Train: Epoch[ 9/10]  [ 0/36]  eta: 0:02:08  Lr: 0.000119  Loss: 0.6336  Acc@1: 79.6875 (79.6875)  Acc@5: 90.6250 (90.6250)  time: 3.5801  data: 0.8406  max mem: 7055
Train: Epoch[ 9/10]  [10/36]  eta: 0:01:05  Lr: 0.000119  Loss: 0.6909  Acc@1: 73.4375 (74.2898)  Acc@5: 90.6250 (87.6420)  time: 2.5211  data: 0.0766  max mem: 7055
Train: Epoch[ 9/10]  [20/36]  eta: 0:00:39  Lr: 0.000119  Loss: 0.8555  Acc@1: 70.3125 (72.0238)  Acc@5: 85.9375 (86.4583)  time: 2.4069  data: 0.0002  max mem: 7055
Train: Epoch[ 9/10]  [30/36]  eta: 0:00:14  Lr: 0.000119  Loss: 0.5840  Acc@1: 70.3125 (71.2702)  Acc@5: 84.3750 (85.7863)  time: 2.3933  data: 0.0003  max mem: 7055
Train: Epoch[ 9/10]  [35/36]  eta: 0:00:02  Lr: 0.000119  Loss: 0.5912  Acc@1: 71.8750 (71.7982)  Acc@5: 85.0000 (86.0965)  time: 2.3589  data: 0.0002  max mem: 7055
Train: Epoch[ 9/10] Total time: 0:01:27 (2.4177 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.5912  Acc@1: 71.8750 (71.7982)  Acc@5: 85.0000 (86.0965)
Train: Epoch[10/10]  [ 0/36]  eta: 0:02:16  Lr: 0.000031  Loss: 0.5390  Acc@1: 76.5625 (76.5625)  Acc@5: 87.5000 (87.5000)  time: 3.7918  data: 0.9432  max mem: 7055
Train: Epoch[10/10]  [10/36]  eta: 0:01:05  Lr: 0.000031  Loss: 0.5998  Acc@1: 71.8750 (71.7330)  Acc@5: 85.9375 (86.2216)  time: 2.5240  data: 0.0860  max mem: 7055
Train: Epoch[10/10]  [20/36]  eta: 0:00:39  Lr: 0.000031  Loss: 0.5956  Acc@1: 71.8750 (72.0238)  Acc@5: 85.9375 (86.3095)  time: 2.3953  data: 0.0002  max mem: 7055
Train: Epoch[10/10]  [30/36]  eta: 0:00:14  Lr: 0.000031  Loss: 0.5696  Acc@1: 71.8750 (71.4214)  Acc@5: 85.9375 (86.3911)  time: 2.3912  data: 0.0002  max mem: 7055
Train: Epoch[10/10]  [35/36]  eta: 0:00:02  Lr: 0.000031  Loss: 1.0348  Acc@1: 71.8750 (71.7544)  Acc@5: 84.3750 (86.2281)  time: 2.3594  data: 0.0002  max mem: 7055
Train: Epoch[10/10] Total time: 0:01:27 (2.4183 s / it)
Averaged stats: Lr: 0.000031  Loss: 1.0348  Acc@1: 71.8750 (71.7544)  Acc@5: 84.3750 (86.2281)
Test: [Task 1]  [ 0/11]  eta: 0:00:19  Loss: 0.5764 (0.5764)  Acc@1: 85.9375 (85.9375)  Acc@5: 95.3125 (95.3125)  time: 1.7403  data: 0.8239  max mem: 7055
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.7398 (0.7223)  Acc@1: 82.8125 (83.8663)  Acc@5: 95.3125 (94.0407)  time: 0.7787  data: 0.0751  max mem: 7055
Test: [Task 1] Total time: 0:00:08 (0.7862 s / it)
* Acc@1 83.866 Acc@5 94.041 loss 0.722
Batchwise eval time for task 1 = 0.7862672805786133
Test: [Task 2]  [ 0/12]  eta: 0:00:22  Loss: 0.3654 (0.3654)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.9124  data: 1.0079  max mem: 7055
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.7119 (0.8167)  Acc@1: 81.2500 (81.8182)  Acc@5: 93.7500 (91.9034)  time: 0.8031  data: 0.0921  max mem: 7055
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.6789 (0.7489)  Acc@1: 81.2500 (81.8440)  Acc@5: 93.7500 (91.9149)  time: 0.7549  data: 0.0844  max mem: 7055
Test: [Task 2] Total time: 0:00:09 (0.7621 s / it)
* Acc@1 81.844 Acc@5 91.915 loss 0.749
Batchwise eval time for task 2 = 0.7621232271194458
Test: [Task 3]  [0/7]  eta: 0:00:12  Loss: 1.2248 (1.2248)  Acc@1: 73.4375 (73.4375)  Acc@5: 85.9375 (85.9375)  time: 1.7669  data: 1.0083  max mem: 7055
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 1.0459 (1.1541)  Acc@1: 75.0000 (73.9631)  Acc@5: 92.1875 (90.7834)  time: 0.8335  data: 0.1442  max mem: 7055
Test: [Task 3] Total time: 0:00:05 (0.8462 s / it)
* Acc@1 73.963 Acc@5 90.783 loss 1.154
Batchwise eval time for task 3 = 0.8462197780609131
Test: [Task 4]  [ 0/10]  eta: 0:00:17  Loss: 1.0841 (1.0841)  Acc@1: 78.1250 (78.1250)  Acc@5: 90.6250 (90.6250)  time: 1.7198  data: 0.8770  max mem: 7055
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 1.0841 (1.1074)  Acc@1: 73.4375 (74.5484)  Acc@5: 89.0625 (89.6552)  time: 0.7672  data: 0.0879  max mem: 7055
Test: [Task 4] Total time: 0:00:07 (0.7751 s / it)
* Acc@1 74.548 Acc@5 89.655 loss 1.107
Batchwise eval time for task 4 = 0.7750765562057496
Test: [Task 5]  [ 0/11]  eta: 0:00:21  Loss: 0.8786 (0.8786)  Acc@1: 82.8125 (82.8125)  Acc@5: 92.1875 (92.1875)  time: 1.9653  data: 0.9693  max mem: 7055
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 0.8807 (0.9777)  Acc@1: 78.1250 (77.6758)  Acc@5: 90.6250 (90.8257)  time: 0.7722  data: 0.0883  max mem: 7055
Test: [Task 5] Total time: 0:00:08 (0.7798 s / it)
* Acc@1 77.676 Acc@5 90.826 loss 0.978
Batchwise eval time for task 5 = 0.7798128128051758
Test: [Task 6]  [0/9]  eta: 0:00:16  Loss: 1.3866 (1.3866)  Acc@1: 70.3125 (70.3125)  Acc@5: 89.0625 (89.0625)  time: 1.8672  data: 1.0114  max mem: 7055
Test: [Task 6]  [8/9]  eta: 0:00:00  Loss: 0.9008 (0.9068)  Acc@1: 79.6875 (79.4918)  Acc@5: 90.6250 (92.5590)  time: 0.7963  data: 0.1126  max mem: 7055
Test: [Task 6] Total time: 0:00:07 (0.8062 s / it)
* Acc@1 79.492 Acc@5 92.559 loss 0.907
Batchwise eval time for task 6 = 0.8062256971995035
Test: [Task 7]  [0/9]  eta: 0:00:22  Loss: 0.9796 (0.9796)  Acc@1: 79.6875 (79.6875)  Acc@5: 90.6250 (90.6250)  time: 2.4510  data: 1.7357  max mem: 7055
Test: [Task 7]  [8/9]  eta: 0:00:00  Loss: 1.0309 (1.1268)  Acc@1: 78.1250 (76.1566)  Acc@5: 90.6250 (89.6797)  time: 0.8783  data: 0.1930  max mem: 7055
Test: [Task 7] Total time: 0:00:08 (0.8889 s / it)
* Acc@1 76.157 Acc@5 89.680 loss 1.127
Batchwise eval time for task 7 = 0.8889393541547987
[Average accuracy till task7]	Acc@1: 78.5867	Acc@5: 91.3512	Loss: 0.9634	Forgetting: 2.7566	Backward: -2.6856
Eval time for task 7 = 55.536765813827515
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.7793)  Task:  7
Old Num K:  12 New Num K:  14
Task number:  7
Train: Epoch[ 1/10]  [ 0/29]  eta: 0:01:49  Lr: 0.001250  Loss: 3.2042  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 3.7765  data: 0.7979  max mem: 7148
Train: Epoch[ 1/10]  [10/29]  eta: 0:00:52  Lr: 0.001250  Loss: 2.7953  Acc@1: 0.0000 (0.4261)  Acc@5: 1.5625 (8.0966)  time: 2.7445  data: 0.0727  max mem: 7159
Train: Epoch[ 1/10]  [20/29]  eta: 0:00:24  Lr: 0.001250  Loss: 3.0571  Acc@1: 1.5625 (7.3661)  Acc@5: 21.8750 (25.2976)  time: 2.6491  data: 0.0002  max mem: 7159
Train: Epoch[ 1/10]  [28/29]  eta: 0:00:02  Lr: 0.001250  Loss: 2.4471  Acc@1: 20.3125 (13.3333)  Acc@5: 50.0000 (34.2778)  time: 2.5948  data: 0.0002  max mem: 7159
Train: Epoch[ 1/10] Total time: 0:01:16 (2.6530 s / it)
Averaged stats: Lr: 0.001250  Loss: 2.4471  Acc@1: 20.3125 (13.3333)  Acc@5: 50.0000 (34.2778)
Train: Epoch[ 2/10]  [ 0/29]  eta: 0:01:49  Lr: 0.001219  Loss: 2.4200  Acc@1: 31.2500 (31.2500)  Acc@5: 67.1875 (67.1875)  time: 3.7838  data: 0.8355  max mem: 7159
Train: Epoch[ 2/10]  [10/29]  eta: 0:00:52  Lr: 0.001219  Loss: 1.4300  Acc@1: 42.1875 (43.1818)  Acc@5: 67.1875 (69.6023)  time: 2.7511  data: 0.0761  max mem: 7160
Train: Epoch[ 2/10]  [20/29]  eta: 0:00:24  Lr: 0.001219  Loss: 1.7048  Acc@1: 48.4375 (46.5774)  Acc@5: 73.4375 (71.5030)  time: 2.6435  data: 0.0002  max mem: 7160
Train: Epoch[ 2/10]  [28/29]  eta: 0:00:02  Lr: 0.001219  Loss: 1.8618  Acc@1: 51.5625 (48.5000)  Acc@5: 75.0000 (72.6667)  time: 2.5673  data: 0.0002  max mem: 7160
Train: Epoch[ 2/10] Total time: 0:01:16 (2.6377 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.8618  Acc@1: 51.5625 (48.5000)  Acc@5: 75.0000 (72.6667)
Train: Epoch[ 3/10]  [ 0/29]  eta: 0:01:50  Lr: 0.001131  Loss: 1.1710  Acc@1: 59.3750 (59.3750)  Acc@5: 81.2500 (81.2500)  time: 3.7978  data: 0.8245  max mem: 7160
Train: Epoch[ 3/10]  [10/29]  eta: 0:00:51  Lr: 0.001131  Loss: 1.8129  Acc@1: 60.9375 (58.8068)  Acc@5: 78.1250 (77.2727)  time: 2.7324  data: 0.0752  max mem: 7160
Train: Epoch[ 3/10]  [20/29]  eta: 0:00:24  Lr: 0.001131  Loss: 1.3400  Acc@1: 60.9375 (59.0774)  Acc@5: 78.1250 (77.8274)  time: 2.6335  data: 0.0003  max mem: 7160
Train: Epoch[ 3/10]  [28/29]  eta: 0:00:02  Lr: 0.001131  Loss: 1.7614  Acc@1: 60.9375 (59.7778)  Acc@5: 78.1250 (77.6667)  time: 2.5612  data: 0.0002  max mem: 7160
Train: Epoch[ 3/10] Total time: 0:01:16 (2.6258 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.7614  Acc@1: 60.9375 (59.7778)  Acc@5: 78.1250 (77.6667)
Train: Epoch[ 4/10]  [ 0/29]  eta: 0:01:54  Lr: 0.000992  Loss: 1.4493  Acc@1: 53.1250 (53.1250)  Acc@5: 75.0000 (75.0000)  time: 3.9650  data: 0.8818  max mem: 7160
Train: Epoch[ 4/10]  [10/29]  eta: 0:00:52  Lr: 0.000992  Loss: 1.2321  Acc@1: 60.9375 (62.6420)  Acc@5: 82.8125 (82.6705)  time: 2.7440  data: 0.0804  max mem: 7160
Train: Epoch[ 4/10]  [20/29]  eta: 0:00:24  Lr: 0.000992  Loss: 1.4266  Acc@1: 62.5000 (62.9464)  Acc@5: 82.8125 (82.1429)  time: 2.6327  data: 0.0003  max mem: 7160
Train: Epoch[ 4/10]  [28/29]  eta: 0:00:02  Lr: 0.000992  Loss: 1.1489  Acc@1: 65.6250 (64.5000)  Acc@5: 81.2500 (82.1667)  time: 2.5598  data: 0.0002  max mem: 7160
Train: Epoch[ 4/10] Total time: 0:01:16 (2.6290 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.1489  Acc@1: 65.6250 (64.5000)  Acc@5: 81.2500 (82.1667)
Train: Epoch[ 5/10]  [ 0/29]  eta: 0:01:48  Lr: 0.000818  Loss: 1.4986  Acc@1: 62.5000 (62.5000)  Acc@5: 82.8125 (82.8125)  time: 3.7251  data: 0.6862  max mem: 7160
Train: Epoch[ 5/10]  [10/29]  eta: 0:00:52  Lr: 0.000818  Loss: 1.1122  Acc@1: 67.1875 (66.6193)  Acc@5: 84.3750 (84.8011)  time: 2.7423  data: 0.0626  max mem: 7160
Train: Epoch[ 5/10]  [20/29]  eta: 0:00:24  Lr: 0.000818  Loss: 0.9663  Acc@1: 67.1875 (65.6994)  Acc@5: 84.3750 (84.1518)  time: 2.6444  data: 0.0002  max mem: 7160
Train: Epoch[ 5/10]  [28/29]  eta: 0:00:02  Lr: 0.000818  Loss: 1.1349  Acc@1: 67.1875 (66.0000)  Acc@5: 84.3750 (83.8889)  time: 2.5630  data: 0.0002  max mem: 7160
Train: Epoch[ 5/10] Total time: 0:01:16 (2.6304 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.1349  Acc@1: 67.1875 (66.0000)  Acc@5: 84.3750 (83.8889)
Train: Epoch[ 6/10]  [ 0/29]  eta: 0:01:47  Lr: 0.000625  Loss: 1.2931  Acc@1: 65.6250 (65.6250)  Acc@5: 79.6875 (79.6875)  time: 3.7077  data: 0.7370  max mem: 7160
Train: Epoch[ 6/10]  [10/29]  eta: 0:00:51  Lr: 0.000625  Loss: 0.8317  Acc@1: 70.3125 (69.0341)  Acc@5: 85.9375 (84.2330)  time: 2.7289  data: 0.0672  max mem: 7160
Train: Epoch[ 6/10]  [20/29]  eta: 0:00:24  Lr: 0.000625  Loss: 1.0666  Acc@1: 70.3125 (70.0149)  Acc@5: 85.9375 (85.4911)  time: 2.6406  data: 0.0002  max mem: 7160
Train: Epoch[ 6/10]  [28/29]  eta: 0:00:02  Lr: 0.000625  Loss: 1.1100  Acc@1: 68.7500 (69.2222)  Acc@5: 85.9375 (85.1111)  time: 2.5686  data: 0.0002  max mem: 7160
Train: Epoch[ 6/10] Total time: 0:01:16 (2.6300 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.1100  Acc@1: 68.7500 (69.2222)  Acc@5: 85.9375 (85.1111)
Train: Epoch[ 7/10]  [ 0/29]  eta: 0:01:51  Lr: 0.000432  Loss: 0.8348  Acc@1: 67.1875 (67.1875)  Acc@5: 89.0625 (89.0625)  time: 3.8412  data: 0.7799  max mem: 7160
Train: Epoch[ 7/10]  [10/29]  eta: 0:00:52  Lr: 0.000432  Loss: 0.6574  Acc@1: 71.8750 (70.5966)  Acc@5: 84.3750 (85.3693)  time: 2.7386  data: 0.0711  max mem: 7160
Train: Epoch[ 7/10]  [20/29]  eta: 0:00:24  Lr: 0.000432  Loss: 0.7012  Acc@1: 68.7500 (69.3452)  Acc@5: 84.3750 (84.9702)  time: 2.6460  data: 0.0002  max mem: 7160
Train: Epoch[ 7/10]  [28/29]  eta: 0:00:02  Lr: 0.000432  Loss: 0.6728  Acc@1: 70.3125 (69.6111)  Acc@5: 84.3750 (84.7778)  time: 2.5761  data: 0.0002  max mem: 7160
Train: Epoch[ 7/10] Total time: 0:01:16 (2.6386 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.6728  Acc@1: 70.3125 (69.6111)  Acc@5: 84.3750 (84.7778)
Train: Epoch[ 8/10]  [ 0/29]  eta: 0:01:50  Lr: 0.000258  Loss: 0.7162  Acc@1: 73.4375 (73.4375)  Acc@5: 90.6250 (90.6250)  time: 3.8206  data: 0.7174  max mem: 7160
Train: Epoch[ 8/10]  [10/29]  eta: 0:00:52  Lr: 0.000258  Loss: 0.4757  Acc@1: 70.3125 (68.7500)  Acc@5: 85.9375 (85.6534)  time: 2.7437  data: 0.0654  max mem: 7160
Train: Epoch[ 8/10]  [20/29]  eta: 0:00:24  Lr: 0.000258  Loss: 0.5045  Acc@1: 70.3125 (69.1220)  Acc@5: 84.3750 (85.6399)  time: 2.6469  data: 0.0002  max mem: 7160
Train: Epoch[ 8/10]  [28/29]  eta: 0:00:02  Lr: 0.000258  Loss: 1.6754  Acc@1: 70.3125 (69.2778)  Acc@5: 84.3750 (85.0000)  time: 2.5715  data: 0.0002  max mem: 7160
Train: Epoch[ 8/10] Total time: 0:01:16 (2.6374 s / it)
Averaged stats: Lr: 0.000258  Loss: 1.6754  Acc@1: 70.3125 (69.2778)  Acc@5: 84.3750 (85.0000)
Train: Epoch[ 9/10]  [ 0/29]  eta: 0:01:51  Lr: 0.000119  Loss: 0.8687  Acc@1: 59.3750 (59.3750)  Acc@5: 82.8125 (82.8125)  time: 3.8334  data: 0.7901  max mem: 7160
Train: Epoch[ 9/10]  [10/29]  eta: 0:00:52  Lr: 0.000119  Loss: 0.4860  Acc@1: 73.4375 (71.8750)  Acc@5: 85.9375 (86.7898)  time: 2.7518  data: 0.0720  max mem: 7160
Train: Epoch[ 9/10]  [20/29]  eta: 0:00:24  Lr: 0.000119  Loss: 0.5046  Acc@1: 70.3125 (70.6101)  Acc@5: 84.3750 (85.5655)  time: 2.6521  data: 0.0002  max mem: 7160
Train: Epoch[ 9/10]  [28/29]  eta: 0:00:02  Lr: 0.000119  Loss: 0.3301  Acc@1: 68.7500 (71.2778)  Acc@5: 85.9375 (85.8889)  time: 2.5756  data: 0.0002  max mem: 7160
Train: Epoch[ 9/10] Total time: 0:01:16 (2.6429 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.3301  Acc@1: 68.7500 (71.2778)  Acc@5: 85.9375 (85.8889)
Train: Epoch[10/10]  [ 0/29]  eta: 0:01:53  Lr: 0.000031  Loss: 0.6731  Acc@1: 64.0625 (64.0625)  Acc@5: 84.3750 (84.3750)  time: 3.8974  data: 0.8004  max mem: 7160
Train: Epoch[10/10]  [10/29]  eta: 0:00:52  Lr: 0.000031  Loss: 0.5919  Acc@1: 67.1875 (68.0398)  Acc@5: 85.9375 (85.0852)  time: 2.7530  data: 0.0730  max mem: 7160
Train: Epoch[10/10]  [20/29]  eta: 0:00:24  Lr: 0.000031  Loss: 0.6557  Acc@1: 67.1875 (68.9732)  Acc@5: 85.9375 (86.0119)  time: 2.6497  data: 0.0002  max mem: 7160
Train: Epoch[10/10]  [28/29]  eta: 0:00:02  Lr: 0.000031  Loss: 0.2684  Acc@1: 70.3125 (69.6111)  Acc@5: 87.5000 (86.3889)  time: 2.5760  data: 0.0002  max mem: 7160
Train: Epoch[10/10] Total time: 0:01:16 (2.6424 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.2684  Acc@1: 70.3125 (69.6111)  Acc@5: 87.5000 (86.3889)
Test: [Task 1]  [ 0/11]  eta: 0:00:20  Loss: 0.6283 (0.6283)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.3125 (95.3125)  time: 1.8784  data: 0.8565  max mem: 7160
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.7646 (0.7474)  Acc@1: 84.3750 (83.7209)  Acc@5: 95.3125 (94.1860)  time: 0.8378  data: 0.0781  max mem: 7160
Test: [Task 1] Total time: 0:00:09 (0.8455 s / it)
* Acc@1 83.721 Acc@5 94.186 loss 0.747
Batchwise eval time for task 1 = 0.8455692421306263
Test: [Task 2]  [ 0/12]  eta: 0:00:23  Loss: 0.3850 (0.3850)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.9370  data: 0.9885  max mem: 7160
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.7641 (0.8540)  Acc@1: 79.6875 (80.9659)  Acc@5: 92.1875 (91.6193)  time: 0.8525  data: 0.0901  max mem: 7160
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.7388 (0.7832)  Acc@1: 79.6875 (80.9929)  Acc@5: 92.1875 (91.6312)  time: 0.8049  data: 0.0826  max mem: 7160
Test: [Task 2] Total time: 0:00:09 (0.8128 s / it)
* Acc@1 80.993 Acc@5 91.631 loss 0.783
Batchwise eval time for task 2 = 0.8128674030303955
Test: [Task 3]  [0/7]  eta: 0:00:12  Loss: 1.2526 (1.2526)  Acc@1: 71.8750 (71.8750)  Acc@5: 85.9375 (85.9375)  time: 1.8566  data: 1.0556  max mem: 7160
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 1.0624 (1.1766)  Acc@1: 76.5625 (73.2719)  Acc@5: 92.1875 (90.7834)  time: 0.8894  data: 0.1510  max mem: 7160
Test: [Task 3] Total time: 0:00:06 (0.9022 s / it)
* Acc@1 73.272 Acc@5 90.783 loss 1.177
Batchwise eval time for task 3 = 0.9022336346762521
Test: [Task 4]  [ 0/10]  eta: 0:00:18  Loss: 1.0935 (1.0935)  Acc@1: 76.5625 (76.5625)  Acc@5: 89.0625 (89.0625)  time: 1.8518  data: 0.9101  max mem: 7160
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 1.0935 (1.1369)  Acc@1: 73.4375 (73.2348)  Acc@5: 89.0625 (89.1626)  time: 0.8243  data: 0.0912  max mem: 7160
Test: [Task 4] Total time: 0:00:08 (0.8340 s / it)
* Acc@1 73.235 Acc@5 89.163 loss 1.137
Batchwise eval time for task 4 = 0.833986759185791
Test: [Task 5]  [ 0/11]  eta: 0:00:19  Loss: 0.8864 (0.8864)  Acc@1: 84.3750 (84.3750)  Acc@5: 92.1875 (92.1875)  time: 1.7979  data: 0.9735  max mem: 7160
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 0.9333 (1.0192)  Acc@1: 78.1250 (77.8287)  Acc@5: 90.6250 (90.0612)  time: 0.8049  data: 0.0887  max mem: 7160
Test: [Task 5] Total time: 0:00:08 (0.8127 s / it)
* Acc@1 77.829 Acc@5 90.061 loss 1.019
Batchwise eval time for task 5 = 0.8127245903015137
Test: [Task 6]  [0/9]  eta: 0:00:16  Loss: 1.4300 (1.4300)  Acc@1: 68.7500 (68.7500)  Acc@5: 89.0625 (89.0625)  time: 1.8084  data: 0.9534  max mem: 7160
Test: [Task 6]  [8/9]  eta: 0:00:00  Loss: 0.8987 (0.9387)  Acc@1: 79.6875 (78.4029)  Acc@5: 90.6250 (92.0145)  time: 0.8376  data: 0.1061  max mem: 7160
Test: [Task 6] Total time: 0:00:07 (0.8475 s / it)
* Acc@1 78.403 Acc@5 92.015 loss 0.939
Batchwise eval time for task 6 = 0.8474906815422906
Test: [Task 7]  [0/9]  eta: 0:00:22  Loss: 1.0202 (1.0202)  Acc@1: 81.2500 (81.2500)  Acc@5: 90.6250 (90.6250)  time: 2.4930  data: 1.7274  max mem: 7160
Test: [Task 7]  [8/9]  eta: 0:00:00  Loss: 1.0423 (1.1780)  Acc@1: 78.1250 (75.6228)  Acc@5: 89.0625 (89.1459)  time: 0.9271  data: 0.1922  max mem: 7160
Test: [Task 7] Total time: 0:00:08 (0.9374 s / it)
* Acc@1 75.623 Acc@5 89.146 loss 1.178
Batchwise eval time for task 7 = 0.9374403158823649
Test: [Task 8]  [0/8]  eta: 0:00:16  Loss: 1.5764 (1.5764)  Acc@1: 64.0625 (64.0625)  Acc@5: 79.6875 (79.6875)  time: 2.0391  data: 1.1974  max mem: 7160
Test: [Task 8]  [7/8]  eta: 0:00:00  Loss: 0.9926 (1.1284)  Acc@1: 75.8621 (75.6813)  Acc@5: 89.0625 (87.2117)  time: 0.9174  data: 0.1498  max mem: 7160
Test: [Task 8] Total time: 0:00:07 (0.9293 s / it)
* Acc@1 75.681 Acc@5 87.212 loss 1.128
Batchwise eval time for task 8 = 0.9293254911899567
[Average accuracy till task8]	Acc@1: 77.7350	Acc@5: 90.5246	Loss: 1.0135	Forgetting: 3.0015	Backward: -2.9407
Eval time for task 8 = 66.43518424034119
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.7824)  Task:  8
Old Num K:  14 New Num K:  16
Task number:  8
Train: Epoch[ 1/10]  [ 0/40]  eta: 0:02:53  Lr: 0.001250  Loss: 3.1693  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 4.3491  data: 1.1420  max mem: 7254
Train: Epoch[ 1/10]  [10/40]  eta: 0:01:30  Lr: 0.001250  Loss: 2.1364  Acc@1: 3.1250 (3.6932)  Acc@5: 12.5000 (14.4886)  time: 3.0233  data: 0.1040  max mem: 7267
Train: Epoch[ 1/10]  [20/40]  eta: 0:00:59  Lr: 0.001250  Loss: 3.1074  Acc@1: 9.3750 (12.5744)  Acc@5: 37.5000 (33.5565)  time: 2.9000  data: 0.0002  max mem: 7267
Train: Epoch[ 1/10]  [30/40]  eta: 0:00:29  Lr: 0.001250  Loss: 1.6409  Acc@1: 31.2500 (22.5806)  Acc@5: 65.6250 (45.8165)  time: 2.8953  data: 0.0002  max mem: 7267
Train: Epoch[ 1/10]  [39/40]  eta: 0:00:02  Lr: 0.001250  Loss: 1.7950  Acc@1: 43.7500 (28.8800)  Acc@5: 70.3125 (51.4800)  time: 2.8287  data: 0.0002  max mem: 7267
Train: Epoch[ 1/10] Total time: 0:01:56 (2.9044 s / it)
Averaged stats: Lr: 0.001250  Loss: 1.7950  Acc@1: 43.7500 (28.8800)  Acc@5: 70.3125 (51.4800)
Train: Epoch[ 2/10]  [ 0/40]  eta: 0:02:50  Lr: 0.001219  Loss: 1.7400  Acc@1: 57.8125 (57.8125)  Acc@5: 85.9375 (85.9375)  time: 4.2681  data: 1.0500  max mem: 7267
Train: Epoch[ 2/10]  [10/40]  eta: 0:01:30  Lr: 0.001219  Loss: 1.2779  Acc@1: 57.8125 (56.9602)  Acc@5: 78.1250 (78.5511)  time: 3.0096  data: 0.0956  max mem: 7267
Train: Epoch[ 2/10]  [20/40]  eta: 0:00:59  Lr: 0.001219  Loss: 1.6551  Acc@1: 59.3750 (59.0774)  Acc@5: 76.5625 (78.2738)  time: 2.8926  data: 0.0002  max mem: 7267
Train: Epoch[ 2/10]  [30/40]  eta: 0:00:29  Lr: 0.001219  Loss: 1.2225  Acc@1: 64.0625 (61.1391)  Acc@5: 82.8125 (79.8891)  time: 2.9004  data: 0.0002  max mem: 7267
Train: Epoch[ 2/10]  [39/40]  eta: 0:00:02  Lr: 0.001219  Loss: 1.5950  Acc@1: 65.6250 (62.3600)  Acc@5: 84.3750 (81.2400)  time: 2.8219  data: 0.0002  max mem: 7267
Train: Epoch[ 2/10] Total time: 0:01:55 (2.8947 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.5950  Acc@1: 65.6250 (62.3600)  Acc@5: 84.3750 (81.2400)
Train: Epoch[ 3/10]  [ 0/40]  eta: 0:02:44  Lr: 0.001131  Loss: 1.4860  Acc@1: 65.6250 (65.6250)  Acc@5: 85.9375 (85.9375)  time: 4.1162  data: 0.9444  max mem: 7267
Train: Epoch[ 3/10]  [10/40]  eta: 0:01:30  Lr: 0.001131  Loss: 0.9809  Acc@1: 68.7500 (68.6080)  Acc@5: 82.8125 (83.9489)  time: 3.0011  data: 0.0861  max mem: 7267
Train: Epoch[ 3/10]  [20/40]  eta: 0:00:59  Lr: 0.001131  Loss: 1.6484  Acc@1: 68.7500 (69.5685)  Acc@5: 82.8125 (84.0774)  time: 2.9041  data: 0.0002  max mem: 7267
Train: Epoch[ 3/10]  [30/40]  eta: 0:00:29  Lr: 0.001131  Loss: 1.0884  Acc@1: 70.3125 (70.2621)  Acc@5: 85.9375 (84.7782)  time: 2.9024  data: 0.0002  max mem: 7267
Train: Epoch[ 3/10]  [39/40]  eta: 0:00:02  Lr: 0.001131  Loss: 2.5535  Acc@1: 70.3125 (70.6000)  Acc@5: 85.9375 (85.3200)  time: 2.8120  data: 0.0002  max mem: 7267
Train: Epoch[ 3/10] Total time: 0:01:55 (2.8913 s / it)
Averaged stats: Lr: 0.001131  Loss: 2.5535  Acc@1: 70.3125 (70.6000)  Acc@5: 85.9375 (85.3200)
Train: Epoch[ 4/10]  [ 0/40]  eta: 0:02:44  Lr: 0.000992  Loss: 1.5802  Acc@1: 67.1875 (67.1875)  Acc@5: 79.6875 (79.6875)  time: 4.1066  data: 0.7122  max mem: 7267
Train: Epoch[ 4/10]  [10/40]  eta: 0:01:30  Lr: 0.000992  Loss: 0.9411  Acc@1: 71.8750 (71.0227)  Acc@5: 85.9375 (86.2216)  time: 3.0003  data: 0.0649  max mem: 7267
Train: Epoch[ 4/10]  [20/40]  eta: 0:00:59  Lr: 0.000992  Loss: 1.3340  Acc@1: 71.8750 (72.3958)  Acc@5: 87.5000 (86.3839)  time: 2.9038  data: 0.0002  max mem: 7267
Train: Epoch[ 4/10]  [30/40]  eta: 0:00:29  Lr: 0.000992  Loss: 0.9927  Acc@1: 73.4375 (73.3871)  Acc@5: 87.5000 (86.7440)  time: 2.8985  data: 0.0002  max mem: 7267
Train: Epoch[ 4/10]  [39/40]  eta: 0:00:02  Lr: 0.000992  Loss: 0.9669  Acc@1: 75.0000 (73.9200)  Acc@5: 87.5000 (87.6000)  time: 2.7999  data: 0.0002  max mem: 7267
Train: Epoch[ 4/10] Total time: 0:01:55 (2.8853 s / it)
Averaged stats: Lr: 0.000992  Loss: 0.9669  Acc@1: 75.0000 (73.9200)  Acc@5: 87.5000 (87.6000)
Train: Epoch[ 5/10]  [ 0/40]  eta: 0:02:53  Lr: 0.000818  Loss: 1.4669  Acc@1: 73.4375 (73.4375)  Acc@5: 84.3750 (84.3750)  time: 4.3346  data: 0.7631  max mem: 7267
Train: Epoch[ 5/10]  [10/40]  eta: 0:01:30  Lr: 0.000818  Loss: 0.8894  Acc@1: 73.4375 (73.7216)  Acc@5: 87.5000 (87.6420)  time: 3.0212  data: 0.0695  max mem: 7267
Train: Epoch[ 5/10]  [20/40]  eta: 0:00:59  Lr: 0.000818  Loss: 1.0803  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.7976)  time: 2.9030  data: 0.0002  max mem: 7267
Train: Epoch[ 5/10]  [30/40]  eta: 0:00:29  Lr: 0.000818  Loss: 0.7177  Acc@1: 78.1250 (75.8065)  Acc@5: 89.0625 (88.6593)  time: 2.9033  data: 0.0002  max mem: 7267
Train: Epoch[ 5/10]  [39/40]  eta: 0:00:02  Lr: 0.000818  Loss: 0.6662  Acc@1: 78.1250 (75.7600)  Acc@5: 89.0625 (88.7600)  time: 2.8065  data: 0.0002  max mem: 7267
Train: Epoch[ 5/10] Total time: 0:01:55 (2.8930 s / it)
Averaged stats: Lr: 0.000818  Loss: 0.6662  Acc@1: 78.1250 (75.7600)  Acc@5: 89.0625 (88.7600)
Train: Epoch[ 6/10]  [ 0/40]  eta: 0:02:43  Lr: 0.000625  Loss: 1.0726  Acc@1: 79.6875 (79.6875)  Acc@5: 89.0625 (89.0625)  time: 4.0996  data: 0.7031  max mem: 7267
Train: Epoch[ 6/10]  [10/40]  eta: 0:01:30  Lr: 0.000625  Loss: 0.7270  Acc@1: 79.6875 (79.4034)  Acc@5: 89.0625 (89.7727)  time: 3.0133  data: 0.0642  max mem: 7267
Train: Epoch[ 6/10]  [20/40]  eta: 0:00:59  Lr: 0.000625  Loss: 0.8529  Acc@1: 76.5625 (78.1250)  Acc@5: 89.0625 (89.6577)  time: 2.8935  data: 0.0003  max mem: 7267
Train: Epoch[ 6/10]  [30/40]  eta: 0:00:29  Lr: 0.000625  Loss: 0.8317  Acc@1: 76.5625 (78.2258)  Acc@5: 90.6250 (90.1714)  time: 2.8850  data: 0.0002  max mem: 7267
Train: Epoch[ 6/10]  [39/40]  eta: 0:00:02  Lr: 0.000625  Loss: 0.7637  Acc@1: 76.5625 (78.2400)  Acc@5: 89.0625 (89.9200)  time: 2.8026  data: 0.0002  max mem: 7267
Train: Epoch[ 6/10] Total time: 0:01:55 (2.8811 s / it)
Averaged stats: Lr: 0.000625  Loss: 0.7637  Acc@1: 76.5625 (78.2400)  Acc@5: 89.0625 (89.9200)
Train: Epoch[ 7/10]  [ 0/40]  eta: 0:02:44  Lr: 0.000432  Loss: 0.8243  Acc@1: 76.5625 (76.5625)  Acc@5: 92.1875 (92.1875)  time: 4.1223  data: 0.8849  max mem: 7267
Train: Epoch[ 7/10]  [10/40]  eta: 0:01:30  Lr: 0.000432  Loss: 0.8154  Acc@1: 79.6875 (79.9716)  Acc@5: 92.1875 (91.7614)  time: 3.0147  data: 0.0807  max mem: 7267
Train: Epoch[ 7/10]  [20/40]  eta: 0:00:59  Lr: 0.000432  Loss: 0.7973  Acc@1: 78.1250 (79.4643)  Acc@5: 90.6250 (91.0714)  time: 2.8973  data: 0.0002  max mem: 7267
Train: Epoch[ 7/10]  [30/40]  eta: 0:00:29  Lr: 0.000432  Loss: 0.6562  Acc@1: 79.6875 (79.8891)  Acc@5: 90.6250 (91.1290)  time: 2.8937  data: 0.0002  max mem: 7267
Train: Epoch[ 7/10]  [39/40]  eta: 0:00:02  Lr: 0.000432  Loss: 1.1548  Acc@1: 79.6875 (79.6400)  Acc@5: 90.6250 (90.9200)  time: 2.8064  data: 0.0002  max mem: 7267
Train: Epoch[ 7/10] Total time: 0:01:55 (2.8849 s / it)
Averaged stats: Lr: 0.000432  Loss: 1.1548  Acc@1: 79.6875 (79.6400)  Acc@5: 90.6250 (90.9200)
Train: Epoch[ 8/10]  [ 0/40]  eta: 0:02:48  Lr: 0.000258  Loss: 0.7005  Acc@1: 81.2500 (81.2500)  Acc@5: 89.0625 (89.0625)  time: 4.2093  data: 1.0384  max mem: 7267
Train: Epoch[ 8/10]  [10/40]  eta: 0:01:30  Lr: 0.000258  Loss: 0.4768  Acc@1: 79.6875 (79.2614)  Acc@5: 90.6250 (90.7670)  time: 3.0199  data: 0.0946  max mem: 7267
Train: Epoch[ 8/10]  [20/40]  eta: 0:00:59  Lr: 0.000258  Loss: 0.5428  Acc@1: 79.6875 (79.9851)  Acc@5: 90.6250 (91.2202)  time: 2.8950  data: 0.0002  max mem: 7267
Train: Epoch[ 8/10]  [30/40]  eta: 0:00:29  Lr: 0.000258  Loss: 0.6434  Acc@1: 79.6875 (79.7379)  Acc@5: 90.6250 (90.8266)  time: 2.8865  data: 0.0002  max mem: 7267
Train: Epoch[ 8/10]  [39/40]  eta: 0:00:02  Lr: 0.000258  Loss: 0.8223  Acc@1: 79.6875 (79.5200)  Acc@5: 89.0625 (90.7200)  time: 2.8022  data: 0.0002  max mem: 7267
Train: Epoch[ 8/10] Total time: 0:01:55 (2.8842 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.8223  Acc@1: 79.6875 (79.5200)  Acc@5: 89.0625 (90.7200)
Train: Epoch[ 9/10]  [ 0/40]  eta: 0:02:45  Lr: 0.000119  Loss: 0.7233  Acc@1: 71.8750 (71.8750)  Acc@5: 82.8125 (82.8125)  time: 4.1383  data: 0.7936  max mem: 7267
Train: Epoch[ 9/10]  [10/40]  eta: 0:01:30  Lr: 0.000119  Loss: 0.6142  Acc@1: 76.5625 (78.1250)  Acc@5: 90.6250 (89.2045)  time: 3.0179  data: 0.0724  max mem: 7267
Train: Epoch[ 9/10]  [20/40]  eta: 0:00:59  Lr: 0.000119  Loss: 0.6302  Acc@1: 78.1250 (79.0179)  Acc@5: 90.6250 (89.5089)  time: 2.8975  data: 0.0002  max mem: 7267
Train: Epoch[ 9/10]  [30/40]  eta: 0:00:29  Lr: 0.000119  Loss: 0.4507  Acc@1: 79.6875 (79.1331)  Acc@5: 89.0625 (89.4153)  time: 2.8940  data: 0.0002  max mem: 7267
Train: Epoch[ 9/10]  [39/40]  eta: 0:00:02  Lr: 0.000119  Loss: 0.5028  Acc@1: 79.6875 (79.4000)  Acc@5: 90.6250 (89.8000)  time: 2.8066  data: 0.0002  max mem: 7267
Train: Epoch[ 9/10] Total time: 0:01:55 (2.8862 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.5028  Acc@1: 79.6875 (79.4000)  Acc@5: 90.6250 (89.8000)
Train: Epoch[10/10]  [ 0/40]  eta: 0:02:44  Lr: 0.000031  Loss: 0.3603  Acc@1: 84.3750 (84.3750)  Acc@5: 92.1875 (92.1875)  time: 4.1239  data: 1.1149  max mem: 7267
Train: Epoch[10/10]  [10/40]  eta: 0:01:30  Lr: 0.000031  Loss: 0.2268  Acc@1: 84.3750 (83.9489)  Acc@5: 92.1875 (92.7557)  time: 3.0058  data: 0.1015  max mem: 7267
Train: Epoch[10/10]  [20/40]  eta: 0:00:58  Lr: 0.000031  Loss: 0.2654  Acc@1: 81.2500 (82.4405)  Acc@5: 92.1875 (91.9643)  time: 2.8831  data: 0.0002  max mem: 7267
Train: Epoch[10/10]  [30/40]  eta: 0:00:29  Lr: 0.000031  Loss: 0.4151  Acc@1: 78.1250 (81.5524)  Acc@5: 89.0625 (91.4819)  time: 2.8827  data: 0.0002  max mem: 7267
Train: Epoch[10/10]  [39/40]  eta: 0:00:02  Lr: 0.000031  Loss: 1.2785  Acc@1: 78.1250 (81.4400)  Acc@5: 89.0625 (91.4000)  time: 2.8061  data: 0.0002  max mem: 7267
Train: Epoch[10/10] Total time: 0:01:55 (2.8787 s / it)
Averaged stats: Lr: 0.000031  Loss: 1.2785  Acc@1: 78.1250 (81.4400)  Acc@5: 89.0625 (91.4000)
Test: [Task 1]  [ 0/11]  eta: 0:00:21  Loss: 0.6477 (0.6477)  Acc@1: 84.3750 (84.3750)  Acc@5: 92.1875 (92.1875)  time: 1.9485  data: 0.8521  max mem: 7267
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.7909 (0.7705)  Acc@1: 81.2500 (82.7035)  Acc@5: 92.1875 (93.6047)  time: 0.8885  data: 0.0777  max mem: 7267
Test: [Task 1] Total time: 0:00:09 (0.8983 s / it)
* Acc@1 82.703 Acc@5 93.605 loss 0.771
Batchwise eval time for task 1 = 0.8983317071741278
Test: [Task 2]  [ 0/12]  eta: 0:00:22  Loss: 0.4132 (0.4132)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.8858  data: 0.9370  max mem: 7267
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.8183 (0.8896)  Acc@1: 78.1250 (80.9659)  Acc@5: 92.1875 (91.4773)  time: 0.8916  data: 0.0855  max mem: 7267
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.7927 (0.8158)  Acc@1: 78.1250 (80.9929)  Acc@5: 92.1875 (91.4894)  time: 0.8446  data: 0.0784  max mem: 7267
Test: [Task 2] Total time: 0:00:10 (0.8517 s / it)
* Acc@1 80.993 Acc@5 91.489 loss 0.816
Batchwise eval time for task 2 = 0.8517478108406067
Test: [Task 3]  [0/7]  eta: 0:00:13  Loss: 1.2817 (1.2817)  Acc@1: 71.8750 (71.8750)  Acc@5: 85.9375 (85.9375)  time: 1.9555  data: 1.0863  max mem: 7267
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 1.0754 (1.2136)  Acc@1: 73.4375 (72.3502)  Acc@5: 92.0000 (90.5530)  time: 0.9456  data: 0.1554  max mem: 7267
Test: [Task 3] Total time: 0:00:06 (0.9584 s / it)
* Acc@1 72.350 Acc@5 90.553 loss 1.214
Batchwise eval time for task 3 = 0.9584410190582275
Test: [Task 4]  [ 0/10]  eta: 0:00:19  Loss: 1.1083 (1.1083)  Acc@1: 81.2500 (81.2500)  Acc@5: 89.0625 (89.0625)  time: 1.9233  data: 0.8814  max mem: 7267
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 1.1083 (1.1863)  Acc@1: 76.5625 (73.3990)  Acc@5: 87.5000 (88.1773)  time: 0.8737  data: 0.0883  max mem: 7267
Test: [Task 4] Total time: 0:00:08 (0.8819 s / it)
* Acc@1 73.399 Acc@5 88.177 loss 1.186
Batchwise eval time for task 4 = 0.8819083929061889
Test: [Task 5]  [ 0/11]  eta: 0:00:22  Loss: 0.9250 (0.9250)  Acc@1: 82.8125 (82.8125)  Acc@5: 92.1875 (92.1875)  time: 2.0638  data: 1.0550  max mem: 7267
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 1.1287 (1.0981)  Acc@1: 78.1250 (77.3700)  Acc@5: 90.6250 (88.9908)  time: 0.8716  data: 0.0961  max mem: 7267
Test: [Task 5] Total time: 0:00:09 (0.8803 s / it)
* Acc@1 77.370 Acc@5 88.991 loss 1.098
Batchwise eval time for task 5 = 0.8803524754264138
Test: [Task 6]  [0/9]  eta: 0:00:18  Loss: 1.4759 (1.4759)  Acc@1: 67.1875 (67.1875)  Acc@5: 89.0625 (89.0625)  time: 2.0162  data: 0.9201  max mem: 7267
Test: [Task 6]  [8/9]  eta: 0:00:00  Loss: 0.9273 (0.9839)  Acc@1: 79.6875 (78.7659)  Acc@5: 90.6250 (91.6515)  time: 0.8989  data: 0.1024  max mem: 7267
Test: [Task 6] Total time: 0:00:08 (0.9087 s / it)
* Acc@1 78.766 Acc@5 91.652 loss 0.984
Batchwise eval time for task 6 = 0.9087221357557509
Test: [Task 7]  [0/9]  eta: 0:00:22  Loss: 1.0644 (1.0644)  Acc@1: 81.2500 (81.2500)  Acc@5: 92.1875 (92.1875)  time: 2.5384  data: 1.7286  max mem: 7267
Test: [Task 7]  [8/9]  eta: 0:00:00  Loss: 1.0814 (1.2654)  Acc@1: 76.5625 (73.8434)  Acc@5: 87.5000 (87.9004)  time: 0.9785  data: 0.1923  max mem: 7267
Test: [Task 7] Total time: 0:00:08 (0.9888 s / it)
* Acc@1 73.843 Acc@5 87.900 loss 1.265
Batchwise eval time for task 7 = 0.9888734287685819
Test: [Task 8]  [0/8]  eta: 0:00:15  Loss: 1.6288 (1.6288)  Acc@1: 65.6250 (65.6250)  Acc@5: 78.1250 (78.1250)  time: 1.9399  data: 1.0703  max mem: 7267
Test: [Task 8]  [7/8]  eta: 0:00:00  Loss: 1.0605 (1.1855)  Acc@1: 72.4138 (74.2138)  Acc@5: 87.5000 (86.7925)  time: 0.8951  data: 0.1340  max mem: 7267
Test: [Task 8] Total time: 0:00:07 (0.9071 s / it)
* Acc@1 74.214 Acc@5 86.792 loss 1.186
Batchwise eval time for task 8 = 0.9072000086307526
Test: [Task 9]  [ 0/10]  eta: 0:00:18  Loss: 1.2483 (1.2483)  Acc@1: 70.3125 (70.3125)  Acc@5: 87.5000 (87.5000)  time: 1.8509  data: 0.8747  max mem: 7267
Test: [Task 9]  [ 9/10]  eta: 0:00:00  Loss: 0.8520 (0.9131)  Acc@1: 81.2500 (81.6393)  Acc@5: 90.6250 (91.8033)  time: 0.9178  data: 0.0900  max mem: 7267
Test: [Task 9] Total time: 0:00:09 (0.9280 s / it)
* Acc@1 81.639 Acc@5 91.803 loss 0.913
Batchwise eval time for task 9 = 0.9279957771301269
[Average accuracy till task9]	Acc@1: 77.6560	Acc@5: 90.1070	Loss: 1.0480	Forgetting: 3.2660	Backward: -3.2128
Eval time for task 9 = 79.24431467056274
Using adam optimizer
Reinitialising optimizer
Similarity:  tensor(0.7541)  Task:  9
Old Num K:  16 New Num K:  18
Task number:  9
Train: Epoch[ 1/10]  [ 0/45]  eta: 0:03:20  Lr: 0.001250  Loss: 3.2541  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 4.4629  data: 0.7864  max mem: 7358
Train: Epoch[ 1/10]  [10/45]  eta: 0:01:54  Lr: 0.001250  Loss: 2.5296  Acc@1: 0.0000 (0.7102)  Acc@5: 0.0000 (5.2557)  time: 3.2661  data: 0.0717  max mem: 7370
Train: Epoch[ 1/10]  [20/45]  eta: 0:01:20  Lr: 0.001250  Loss: 3.0765  Acc@1: 3.1250 (7.6637)  Acc@5: 20.3125 (19.5685)  time: 3.1402  data: 0.0002  max mem: 7370
Train: Epoch[ 1/10]  [30/45]  eta: 0:00:47  Lr: 0.001250  Loss: 1.8244  Acc@1: 25.0000 (15.6754)  Acc@5: 45.3125 (30.6452)  time: 3.1365  data: 0.0002  max mem: 7370
Train: Epoch[ 1/10]  [40/45]  eta: 0:00:15  Lr: 0.001250  Loss: 2.2705  Acc@1: 37.5000 (21.9893)  Acc@5: 54.6875 (37.6905)  time: 3.1293  data: 0.0002  max mem: 7370
Train: Epoch[ 1/10]  [44/45]  eta: 0:00:03  Lr: 0.001250  Loss: 1.9970  Acc@1: 39.0625 (23.7000)  Acc@5: 59.3750 (39.6180)  time: 3.0706  data: 0.0002  max mem: 7370
Train: Epoch[ 1/10] Total time: 0:02:21 (3.1441 s / it)
Averaged stats: Lr: 0.001250  Loss: 1.9970  Acc@1: 39.0625 (23.7000)  Acc@5: 59.3750 (39.6180)
Train: Epoch[ 2/10]  [ 0/45]  eta: 0:03:19  Lr: 0.001219  Loss: 2.0903  Acc@1: 53.1250 (53.1250)  Acc@5: 68.7500 (68.7500)  time: 4.4437  data: 0.7659  max mem: 7370
Train: Epoch[ 2/10]  [10/45]  eta: 0:01:53  Lr: 0.001219  Loss: 1.4548  Acc@1: 50.0000 (50.1420)  Acc@5: 68.7500 (68.7500)  time: 3.2404  data: 0.0698  max mem: 7370
Train: Epoch[ 2/10]  [20/45]  eta: 0:01:19  Lr: 0.001219  Loss: 1.6892  Acc@1: 50.0000 (51.3393)  Acc@5: 70.3125 (71.5030)  time: 3.1267  data: 0.0002  max mem: 7370
Train: Epoch[ 2/10]  [30/45]  eta: 0:00:47  Lr: 0.001219  Loss: 1.8554  Acc@1: 54.6875 (53.0242)  Acc@5: 73.4375 (72.0262)  time: 3.1209  data: 0.0002  max mem: 7370
Train: Epoch[ 2/10]  [40/45]  eta: 0:00:15  Lr: 0.001219  Loss: 1.6879  Acc@1: 57.8125 (54.6113)  Acc@5: 73.4375 (72.7134)  time: 3.1103  data: 0.0002  max mem: 7370
Train: Epoch[ 2/10]  [44/45]  eta: 0:00:03  Lr: 0.001219  Loss: 1.5571  Acc@1: 59.3750 (55.1468)  Acc@5: 73.4375 (72.8334)  time: 3.0262  data: 0.0002  max mem: 7370
Train: Epoch[ 2/10] Total time: 0:02:20 (3.1124 s / it)
Averaged stats: Lr: 0.001219  Loss: 1.5571  Acc@1: 59.3750 (55.1468)  Acc@5: 73.4375 (72.8334)
Train: Epoch[ 3/10]  [ 0/45]  eta: 0:03:10  Lr: 0.001131  Loss: 0.9710  Acc@1: 68.7500 (68.7500)  Acc@5: 89.0625 (89.0625)  time: 4.2359  data: 0.7620  max mem: 7370
Train: Epoch[ 3/10]  [10/45]  eta: 0:01:52  Lr: 0.001131  Loss: 1.6441  Acc@1: 60.9375 (60.6534)  Acc@5: 78.1250 (79.8295)  time: 3.2247  data: 0.0694  max mem: 7370
Train: Epoch[ 3/10]  [20/45]  eta: 0:01:19  Lr: 0.001131  Loss: 1.3740  Acc@1: 60.9375 (60.9375)  Acc@5: 78.1250 (78.5714)  time: 3.1168  data: 0.0002  max mem: 7370
Train: Epoch[ 3/10]  [30/45]  eta: 0:00:47  Lr: 0.001131  Loss: 1.7478  Acc@1: 60.9375 (61.7944)  Acc@5: 79.6875 (79.1835)  time: 3.1159  data: 0.0002  max mem: 7370
Train: Epoch[ 3/10]  [40/45]  eta: 0:00:15  Lr: 0.001131  Loss: 1.4514  Acc@1: 62.5000 (61.8140)  Acc@5: 78.1250 (78.8491)  time: 3.1137  data: 0.0002  max mem: 7370
Train: Epoch[ 3/10]  [44/45]  eta: 0:00:03  Lr: 0.001131  Loss: 1.0207  Acc@1: 62.5000 (62.1861)  Acc@5: 78.1250 (79.0591)  time: 3.0319  data: 0.0002  max mem: 7370
Train: Epoch[ 3/10] Total time: 0:02:19 (3.1054 s / it)
Averaged stats: Lr: 0.001131  Loss: 1.0207  Acc@1: 62.5000 (62.1861)  Acc@5: 78.1250 (79.0591)
Train: Epoch[ 4/10]  [ 0/45]  eta: 0:03:11  Lr: 0.000992  Loss: 1.1955  Acc@1: 67.1875 (67.1875)  Acc@5: 81.2500 (81.2500)  time: 4.2524  data: 0.7892  max mem: 7370
Train: Epoch[ 4/10]  [10/45]  eta: 0:01:53  Lr: 0.000992  Loss: 1.4653  Acc@1: 65.6250 (65.6250)  Acc@5: 81.2500 (81.8182)  time: 3.2317  data: 0.0719  max mem: 7370
Train: Epoch[ 4/10]  [20/45]  eta: 0:01:19  Lr: 0.000992  Loss: 1.2518  Acc@1: 62.5000 (64.6577)  Acc@5: 81.2500 (81.3244)  time: 3.1194  data: 0.0002  max mem: 7370
Train: Epoch[ 4/10]  [30/45]  eta: 0:00:47  Lr: 0.000992  Loss: 1.5881  Acc@1: 65.6250 (65.2218)  Acc@5: 81.2500 (81.4012)  time: 3.1147  data: 0.0002  max mem: 7370
Train: Epoch[ 4/10]  [40/45]  eta: 0:00:15  Lr: 0.000992  Loss: 1.1568  Acc@1: 68.7500 (65.7012)  Acc@5: 81.2500 (81.7073)  time: 3.1121  data: 0.0002  max mem: 7370
Train: Epoch[ 4/10]  [44/45]  eta: 0:00:03  Lr: 0.000992  Loss: 1.4080  Acc@1: 68.7500 (66.2186)  Acc@5: 82.8125 (81.9950)  time: 3.0335  data: 0.0002  max mem: 7370
Train: Epoch[ 4/10] Total time: 0:02:19 (3.1106 s / it)
Averaged stats: Lr: 0.000992  Loss: 1.4080  Acc@1: 68.7500 (66.2186)  Acc@5: 82.8125 (81.9950)
Train: Epoch[ 5/10]  [ 0/45]  eta: 0:03:11  Lr: 0.000818  Loss: 1.5576  Acc@1: 60.9375 (60.9375)  Acc@5: 73.4375 (73.4375)  time: 4.2621  data: 0.6968  max mem: 7370
Train: Epoch[ 5/10]  [10/45]  eta: 0:01:52  Lr: 0.000818  Loss: 0.8381  Acc@1: 70.3125 (71.0227)  Acc@5: 84.3750 (84.8011)  time: 3.2153  data: 0.0635  max mem: 7370
Train: Epoch[ 5/10]  [20/45]  eta: 0:01:19  Lr: 0.000818  Loss: 1.0278  Acc@1: 70.3125 (70.3869)  Acc@5: 84.3750 (84.3006)  time: 3.1213  data: 0.0002  max mem: 7370
Train: Epoch[ 5/10]  [30/45]  eta: 0:00:47  Lr: 0.000818  Loss: 0.8214  Acc@1: 67.1875 (69.3044)  Acc@5: 84.3750 (83.9718)  time: 3.1244  data: 0.0002  max mem: 7370
Train: Epoch[ 5/10]  [40/45]  eta: 0:00:15  Lr: 0.000818  Loss: 1.0361  Acc@1: 65.6250 (69.2073)  Acc@5: 84.3750 (84.0320)  time: 3.1179  data: 0.0002  max mem: 7370
Train: Epoch[ 5/10]  [44/45]  eta: 0:00:03  Lr: 0.000818  Loss: 1.4811  Acc@1: 68.7500 (68.9777)  Acc@5: 84.3750 (84.0113)  time: 3.0318  data: 0.0002  max mem: 7370
Train: Epoch[ 5/10] Total time: 0:02:19 (3.1100 s / it)
Averaged stats: Lr: 0.000818  Loss: 1.4811  Acc@1: 68.7500 (68.9777)  Acc@5: 84.3750 (84.0113)
Train: Epoch[ 6/10]  [ 0/45]  eta: 0:03:10  Lr: 0.000625  Loss: 0.9292  Acc@1: 75.0000 (75.0000)  Acc@5: 90.6250 (90.6250)  time: 4.2319  data: 0.7777  max mem: 7370
Train: Epoch[ 6/10]  [10/45]  eta: 0:01:52  Lr: 0.000625  Loss: 0.9672  Acc@1: 73.4375 (72.1591)  Acc@5: 87.5000 (86.3636)  time: 3.2237  data: 0.0709  max mem: 7370
Train: Epoch[ 6/10]  [20/45]  eta: 0:01:19  Lr: 0.000625  Loss: 0.9858  Acc@1: 71.8750 (71.3542)  Acc@5: 82.8125 (85.4167)  time: 3.1167  data: 0.0002  max mem: 7370
Train: Epoch[ 6/10]  [30/45]  eta: 0:00:47  Lr: 0.000625  Loss: 0.8183  Acc@1: 71.8750 (71.0685)  Acc@5: 85.9375 (85.6351)  time: 3.1210  data: 0.0002  max mem: 7370
Train: Epoch[ 6/10]  [40/45]  eta: 0:00:15  Lr: 0.000625  Loss: 0.9684  Acc@1: 71.8750 (71.5320)  Acc@5: 85.9375 (86.0137)  time: 3.1204  data: 0.0002  max mem: 7370
Train: Epoch[ 6/10]  [44/45]  eta: 0:00:03  Lr: 0.000625  Loss: 1.1227  Acc@1: 72.7273 (71.4892)  Acc@5: 85.9375 (85.9568)  time: 3.0366  data: 0.0002  max mem: 7370
Train: Epoch[ 6/10] Total time: 0:02:19 (3.1094 s / it)
Averaged stats: Lr: 0.000625  Loss: 1.1227  Acc@1: 72.7273 (71.4892)  Acc@5: 85.9375 (85.9568)
Train: Epoch[ 7/10]  [ 0/45]  eta: 0:03:17  Lr: 0.000432  Loss: 0.9092  Acc@1: 73.4375 (73.4375)  Acc@5: 84.3750 (84.3750)  time: 4.3832  data: 0.7156  max mem: 7370
Train: Epoch[ 7/10]  [10/45]  eta: 0:01:52  Lr: 0.000432  Loss: 0.8372  Acc@1: 73.4375 (73.1534)  Acc@5: 87.5000 (86.6477)  time: 3.2275  data: 0.0652  max mem: 7370
Train: Epoch[ 7/10]  [20/45]  eta: 0:01:19  Lr: 0.000432  Loss: 0.6651  Acc@1: 73.4375 (73.9583)  Acc@5: 89.0625 (88.2440)  time: 3.1185  data: 0.0002  max mem: 7370
Train: Epoch[ 7/10]  [30/45]  eta: 0:00:47  Lr: 0.000432  Loss: 0.5996  Acc@1: 75.0000 (74.7984)  Acc@5: 89.0625 (88.1048)  time: 3.1269  data: 0.0003  max mem: 7370
Train: Epoch[ 7/10]  [40/45]  eta: 0:00:15  Lr: 0.000432  Loss: 0.8545  Acc@1: 75.0000 (74.3140)  Acc@5: 87.5000 (87.6905)  time: 3.1242  data: 0.0002  max mem: 7370
Train: Epoch[ 7/10]  [44/45]  eta: 0:00:03  Lr: 0.000432  Loss: 0.4201  Acc@1: 72.7273 (73.9653)  Acc@5: 87.5000 (87.5133)  time: 3.0434  data: 0.0002  max mem: 7370
Train: Epoch[ 7/10] Total time: 0:02:20 (3.1154 s / it)
Averaged stats: Lr: 0.000432  Loss: 0.4201  Acc@1: 72.7273 (73.9653)  Acc@5: 87.5000 (87.5133)
Train: Epoch[ 8/10]  [ 0/45]  eta: 0:03:13  Lr: 0.000258  Loss: 0.5201  Acc@1: 81.2500 (81.2500)  Acc@5: 95.3125 (95.3125)  time: 4.2999  data: 0.7318  max mem: 7370
Train: Epoch[ 8/10]  [10/45]  eta: 0:01:53  Lr: 0.000258  Loss: 0.7087  Acc@1: 78.1250 (75.7102)  Acc@5: 87.5000 (88.3523)  time: 3.2363  data: 0.0668  max mem: 7370
Train: Epoch[ 8/10]  [20/45]  eta: 0:01:19  Lr: 0.000258  Loss: 0.6034  Acc@1: 73.4375 (73.1399)  Acc@5: 85.9375 (86.9792)  time: 3.1254  data: 0.0002  max mem: 7370
Train: Epoch[ 8/10]  [30/45]  eta: 0:00:47  Lr: 0.000258  Loss: 0.6498  Acc@1: 73.4375 (73.3367)  Acc@5: 85.9375 (86.7944)  time: 3.1242  data: 0.0002  max mem: 7370
Train: Epoch[ 8/10]  [40/45]  eta: 0:00:15  Lr: 0.000258  Loss: 0.5465  Acc@1: 75.0000 (73.8186)  Acc@5: 85.9375 (87.1189)  time: 3.1239  data: 0.0002  max mem: 7370
Train: Epoch[ 8/10]  [44/45]  eta: 0:00:03  Lr: 0.000258  Loss: 0.7895  Acc@1: 75.0000 (73.4347)  Acc@5: 85.9375 (86.8412)  time: 3.0358  data: 0.0002  max mem: 7370
Train: Epoch[ 8/10] Total time: 0:02:20 (3.1154 s / it)
Averaged stats: Lr: 0.000258  Loss: 0.7895  Acc@1: 75.0000 (73.4347)  Acc@5: 85.9375 (86.8412)
Train: Epoch[ 9/10]  [ 0/45]  eta: 0:03:14  Lr: 0.000119  Loss: 0.6544  Acc@1: 79.6875 (79.6875)  Acc@5: 85.9375 (85.9375)  time: 4.3255  data: 0.8571  max mem: 7370
Train: Epoch[ 9/10]  [10/45]  eta: 0:01:53  Lr: 0.000119  Loss: 0.6020  Acc@1: 73.4375 (74.7159)  Acc@5: 89.0625 (88.9205)  time: 3.2289  data: 0.0781  max mem: 7370
Train: Epoch[ 9/10]  [20/45]  eta: 0:01:19  Lr: 0.000119  Loss: 0.6511  Acc@1: 73.4375 (74.9256)  Acc@5: 89.0625 (88.6905)  time: 3.1175  data: 0.0002  max mem: 7370
Train: Epoch[ 9/10]  [30/45]  eta: 0:00:47  Lr: 0.000119  Loss: 0.5194  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (88.0544)  time: 3.1247  data: 0.0002  max mem: 7370
Train: Epoch[ 9/10]  [40/45]  eta: 0:00:15  Lr: 0.000119  Loss: 0.6518  Acc@1: 76.5625 (75.1524)  Acc@5: 87.5000 (87.8049)  time: 3.1317  data: 0.0002  max mem: 7370
Train: Epoch[ 9/10]  [44/45]  eta: 0:00:03  Lr: 0.000119  Loss: 0.1807  Acc@1: 76.5625 (74.9558)  Acc@5: 87.5000 (87.5840)  time: 3.0417  data: 0.0002  max mem: 7370
Train: Epoch[ 9/10] Total time: 0:02:20 (3.1164 s / it)
Averaged stats: Lr: 0.000119  Loss: 0.1807  Acc@1: 76.5625 (74.9558)  Acc@5: 87.5000 (87.5840)
Train: Epoch[10/10]  [ 0/45]  eta: 0:03:13  Lr: 0.000031  Loss: 0.5061  Acc@1: 76.5625 (76.5625)  Acc@5: 85.9375 (85.9375)  time: 4.3044  data: 0.7355  max mem: 7370
Train: Epoch[10/10]  [10/45]  eta: 0:01:52  Lr: 0.000031  Loss: 0.3044  Acc@1: 70.3125 (72.0170)  Acc@5: 85.9375 (87.7841)  time: 3.2196  data: 0.0671  max mem: 7370
Train: Epoch[10/10]  [20/45]  eta: 0:01:19  Lr: 0.000031  Loss: 0.4918  Acc@1: 71.8750 (73.0655)  Acc@5: 85.9375 (87.1280)  time: 3.1258  data: 0.0002  max mem: 7370
Train: Epoch[10/10]  [30/45]  eta: 0:00:47  Lr: 0.000031  Loss: 0.5176  Acc@1: 73.4375 (72.5806)  Acc@5: 85.9375 (86.9456)  time: 3.1246  data: 0.0002  max mem: 7370
Train: Epoch[10/10]  [40/45]  eta: 0:00:15  Lr: 0.000031  Loss: 0.3111  Acc@1: 73.4375 (73.3994)  Acc@5: 87.5000 (87.4238)  time: 3.1207  data: 0.0002  max mem: 7370
Train: Epoch[10/10]  [44/45]  eta: 0:00:03  Lr: 0.000031  Loss: 0.3250  Acc@1: 73.4375 (73.1871)  Acc@5: 87.5000 (87.4071)  time: 3.0429  data: 0.0002  max mem: 7370
Train: Epoch[10/10] Total time: 0:02:20 (3.1159 s / it)
Averaged stats: Lr: 0.000031  Loss: 0.3250  Acc@1: 73.4375 (73.1871)  Acc@5: 87.5000 (87.4071)
Test: [Task 1]  [ 0/11]  eta: 0:00:22  Loss: 0.6519 (0.6519)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.3125 (95.3125)  time: 2.0500  data: 0.8688  max mem: 7370
Test: [Task 1]  [10/11]  eta: 0:00:00  Loss: 0.8003 (0.7929)  Acc@1: 82.8125 (82.7035)  Acc@5: 93.7500 (93.6047)  time: 0.9427  data: 0.0792  max mem: 7370
Test: [Task 1] Total time: 0:00:10 (0.9504 s / it)
* Acc@1 82.703 Acc@5 93.605 loss 0.793
Batchwise eval time for task 1 = 0.9504846226085316
Test: [Task 2]  [ 0/12]  eta: 0:00:25  Loss: 0.4269 (0.4269)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 2.1346  data: 1.0585  max mem: 7370
Test: [Task 2]  [10/12]  eta: 0:00:01  Loss: 0.8456 (0.9165)  Acc@1: 76.5625 (80.1136)  Acc@5: 92.1875 (90.9091)  time: 0.9602  data: 0.0965  max mem: 7370
Test: [Task 2]  [11/12]  eta: 0:00:00  Loss: 0.8414 (0.8405)  Acc@1: 76.5625 (80.1418)  Acc@5: 92.1875 (90.9220)  time: 0.9082  data: 0.0884  max mem: 7370
Test: [Task 2] Total time: 0:00:10 (0.9155 s / it)
* Acc@1 80.142 Acc@5 90.922 loss 0.840
Batchwise eval time for task 2 = 0.9154792825380961
Test: [Task 3]  [0/7]  eta: 0:00:13  Loss: 1.3341 (1.3341)  Acc@1: 71.8750 (71.8750)  Acc@5: 84.3750 (84.3750)  time: 1.9574  data: 1.0491  max mem: 7370
Test: [Task 3]  [6/7]  eta: 0:00:00  Loss: 1.0867 (1.2391)  Acc@1: 73.4375 (72.1198)  Acc@5: 90.0000 (89.1705)  time: 0.9891  data: 0.1501  max mem: 7370
Test: [Task 3] Total time: 0:00:07 (1.0032 s / it)
* Acc@1 72.120 Acc@5 89.171 loss 1.239
Batchwise eval time for task 3 = 1.0031893934522356
Test: [Task 4]  [ 0/10]  eta: 0:00:19  Loss: 1.1050 (1.1050)  Acc@1: 79.6875 (79.6875)  Acc@5: 89.0625 (89.0625)  time: 1.9681  data: 0.8866  max mem: 7370
Test: [Task 4]  [ 9/10]  eta: 0:00:00  Loss: 1.1050 (1.1966)  Acc@1: 76.5625 (73.7274)  Acc@5: 87.5000 (87.3563)  time: 0.9231  data: 0.0889  max mem: 7370
Test: [Task 4] Total time: 0:00:09 (0.9312 s / it)
* Acc@1 73.727 Acc@5 87.356 loss 1.197
Batchwise eval time for task 4 = 0.9312690734863281
Test: [Task 5]  [ 0/11]  eta: 0:00:22  Loss: 0.9528 (0.9528)  Acc@1: 81.2500 (81.2500)  Acc@5: 92.1875 (92.1875)  time: 2.0336  data: 1.0408  max mem: 7370
Test: [Task 5]  [10/11]  eta: 0:00:00  Loss: 1.1519 (1.1245)  Acc@1: 78.1250 (77.0642)  Acc@5: 90.6250 (88.8379)  time: 0.9130  data: 0.0950  max mem: 7370
Test: [Task 5] Total time: 0:00:10 (0.9207 s / it)
* Acc@1 77.064 Acc@5 88.838 loss 1.124
Batchwise eval time for task 5 = 0.9207670038396661
Test: [Task 6]  [0/9]  eta: 0:00:19  Loss: 1.5188 (1.5188)  Acc@1: 67.1875 (67.1875)  Acc@5: 87.5000 (87.5000)  time: 2.1682  data: 0.9993  max mem: 7370
Test: [Task 6]  [8/9]  eta: 0:00:00  Loss: 1.0052 (1.0217)  Acc@1: 79.6875 (78.0399)  Acc@5: 92.1875 (91.4701)  time: 0.9580  data: 0.1112  max mem: 7370
Test: [Task 6] Total time: 0:00:08 (0.9681 s / it)
* Acc@1 78.040 Acc@5 91.470 loss 1.022
Batchwise eval time for task 6 = 0.9681323104434543
Test: [Task 7]  [0/9]  eta: 0:00:23  Loss: 1.0983 (1.0983)  Acc@1: 81.2500 (81.2500)  Acc@5: 90.6250 (90.6250)  time: 2.5606  data: 1.6989  max mem: 7370
Test: [Task 7]  [8/9]  eta: 0:00:01  Loss: 1.1192 (1.3335)  Acc@1: 75.0000 (71.3523)  Acc@5: 90.6250 (87.7224)  time: 1.0257  data: 0.1889  max mem: 7370
Test: [Task 7] Total time: 0:00:09 (1.0364 s / it)
* Acc@1 71.352 Acc@5 87.722 loss 1.333
Batchwise eval time for task 7 = 1.0364538298712835
Test: [Task 8]  [0/8]  eta: 0:00:16  Loss: 1.6936 (1.6936)  Acc@1: 62.5000 (62.5000)  Acc@5: 78.1250 (78.1250)  time: 2.0153  data: 1.0802  max mem: 7370
Test: [Task 8]  [7/8]  eta: 0:00:00  Loss: 1.1615 (1.2518)  Acc@1: 71.8750 (73.3753)  Acc@5: 89.0625 (86.5828)  time: 0.9461  data: 0.1352  max mem: 7370
Test: [Task 8] Total time: 0:00:07 (0.9564 s / it)
* Acc@1 73.375 Acc@5 86.583 loss 1.252
Batchwise eval time for task 8 = 0.9564703404903412
Test: [Task 9]  [ 0/10]  eta: 0:00:20  Loss: 1.3052 (1.3052)  Acc@1: 70.3125 (70.3125)  Acc@5: 87.5000 (87.5000)  time: 2.0339  data: 0.9301  max mem: 7370
Test: [Task 9]  [ 9/10]  eta: 0:00:00  Loss: 0.8776 (0.9583)  Acc@1: 78.1250 (80.4918)  Acc@5: 90.6250 (91.3115)  time: 0.9333  data: 0.0932  max mem: 7370
Test: [Task 9] Total time: 0:00:09 (0.9429 s / it)
* Acc@1 80.492 Acc@5 91.311 loss 0.958
Batchwise eval time for task 9 = 0.9429387092590332
Test: [Task 10]  [ 0/12]  eta: 0:00:24  Loss: 1.2522 (1.2522)  Acc@1: 71.8750 (71.8750)  Acc@5: 89.0625 (89.0625)  time: 2.0300  data: 0.8973  max mem: 7370
Test: [Task 10]  [10/12]  eta: 0:00:01  Loss: 1.0291 (1.1787)  Acc@1: 75.0000 (76.4205)  Acc@5: 89.0625 (88.0682)  time: 0.9551  data: 0.0819  max mem: 7370
Test: [Task 10]  [11/12]  eta: 0:00:00  Loss: 0.9880 (1.0857)  Acc@1: 75.0000 (76.6197)  Acc@5: 89.0625 (88.1690)  time: 0.9360  data: 0.0751  max mem: 7370
Test: [Task 10] Total time: 0:00:11 (0.9431 s / it)
* Acc@1 76.620 Acc@5 88.169 loss 1.086
Batchwise eval time for task 10 = 0.9431232611338297
[Average accuracy till task10]	Acc@1: 76.9333	Acc@5: 89.5147	Loss: 1.0845	Forgetting: 3.5989	Backward: -3.5516
Eval time for task 10 = 94.67605757713318
Total training time: 2:29:18