method: rainbow
dataset: Split-CIFAR100
data_path: ./data/cifar100
output_dir: ./outputs/rainbow_cifar100

# training setup (match cifar100_convprompt.py defaults)
batch_size: 128
epochs: 10
num_workers: 4
seed: 1996

# continual learning
num_tasks: 10
task_inc: false
train_mask: true

# model backbone
model: vit_base_patch16_224_in21k
pretrained: true
drop: 0.0
drop_path: 0.0
input_size: 224
freeze:
  - blocks
  - patch_embed
  - cls_token
  - pos_embed

# optimizer/scheduler (sync with cifar100_convprompt.py: opt=adam, lr=0.005, min_lr=1e-5)
optimizer:
  name: adam
  lr: 5.0e-3
  weight_decay: 0.0
scheduler:
  name: cosine
  min_lr: 1.0e-5

# prompt hyperparameters
prompt_length: 20
rainbow:
  proj_dim: 96
  align_hidden_dim: 56
  gate_tau_start: 1.0
  gate_tau_end: 0.3
  gate_harden_at: 0.6
  lambda_sparse: 0.01
  lambda_match: 0.01
  save_dir: ./outputs/rainbow_cifar100/prompts
  use_task_conditioning: true
  enable_task_level: true
  enable_feature_level: true
  enable_alignment: true
  use_adaptive_gating: true
  head_lr_multiplier: 2.0  # Learning rate multiplier for classifier head (default: 2.0)

# logging
print_freq: 10

